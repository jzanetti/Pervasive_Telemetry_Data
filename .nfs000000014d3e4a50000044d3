/home/zhangs/miniconda3/envs/airflow/lib/python3.9/site-packages/airflow/models/base.py:49 MovedIn20Warning: Deprecated API features detected! These feature(s) are not compatible with SQLAlchemy 2.0. To prevent incompatible upgrades prior to updating applications, ensure requirements files are pinned to "sqlalchemy<2.0". Set environment variable SQLALCHEMY_WARN_20=1 to show all deprecation warnings.  Set environment variable SQLALCHEMY_SILENCE_UBER_WARNING=1 to silence this message. (Background on SQLAlchemy 2.0 at: https://sqlalche.me/e/b8d9)
  ____________       _____________
 ____    |__( )_________  __/__  /________      __
____  /| |_  /__  ___/_  /_ __  /_  __ \_ | /| / /
___  ___ |  / _  /   _  __/ _  / / /_/ /_ |/ |/ /
 _/_/  |_/_/  /_/    /_/    /_/  \____/____/|__/
[2023-10-27 14:17:54 +1300] [2303192] [INFO] Starting gunicorn 21.2.0
[2023-10-27 14:17:54 +1300] [2303192] [INFO] Listening at: http://[::]:8083 (2303192)
[2023-10-27 14:17:54 +1300] [2303192] [INFO] Using worker: sync
[[34m2023-10-27 14:17:54,569[0m] {[34mscheduler_job.py:[0m700} INFO[0m - Starting the scheduler[0m
[[34m2023-10-27 14:17:54,569[0m] {[34mscheduler_job.py:[0m705} INFO[0m - Processing each file at most -1 times[0m
[[34m2023-10-27 14:17:54,573[0m] {[34mexecutor_loader.py:[0m107} INFO[0m - Loaded executor: SequentialExecutor[0m
[2023-10-27 14:17:54 +1300] [2303222] [INFO] Booting worker with pid: 2303222
[[34m2023-10-27 14:17:54,579[0m] {[34mmanager.py:[0m163} INFO[0m - Launched DagFileProcessorManager with pid: 2303229[0m
[[34m2023-10-27 14:17:54,580[0m] {[34mscheduler_job.py:[0m1380} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2023-10-27 14:17:54,591[0m] {[34msettings.py:[0m58} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2023-10-27T14:17:54.607+1300] {manager.py:409} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2) when using sqlite. So we set parallelism to 1.
[[34m2023-10-27 14:17:54,625[0m] {[34mscheduler_job.py:[0m1403} INFO[0m - Marked 1 SchedulerJob instances as failed[0m
[2023-10-27 14:17:54 +1300] [2303290] [INFO] Booting worker with pid: 2303290
[[34m2023-10-27 14:18:26,002[0m] {[34mscheduler_job.py:[0m346} INFO[0m - 1 tasks up for execution:
	<TaskInstance: a_call_ptd.test_ptd manual__2023-10-27T01:18:24.970010+00:00 [scheduled]>[0m
[[34m2023-10-27 14:18:26,004[0m] {[34mscheduler_job.py:[0m411} INFO[0m - DAG a_call_ptd has 0/16 running and queued tasks[0m
[[34m2023-10-27 14:18:26,006[0m] {[34mscheduler_job.py:[0m497} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: a_call_ptd.test_ptd manual__2023-10-27T01:18:24.970010+00:00 [scheduled]>[0m
[[34m2023-10-27 14:18:26,041[0m] {[34mscheduler_job.py:[0m536} INFO[0m - Sending TaskInstanceKey(dag_id='a_call_ptd', task_id='test_ptd', run_id='manual__2023-10-27T01:18:24.970010+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-10-27 14:18:26,041[0m] {[34mbase_executor.py:[0m95} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'a_call_ptd', 'test_ptd', 'manual__2023-10-27T01:18:24.970010+00:00', '--local', '--subdir', 'DAGS_FOLDER/ptd_airflow.py'][0m
[[34m2023-10-27 14:18:26,060[0m] {[34msequential_executor.py:[0m61} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'a_call_ptd', 'test_ptd', 'manual__2023-10-27T01:18:24.970010+00:00', '--local', '--subdir', 'DAGS_FOLDER/ptd_airflow.py'][0m
/home/zhangs/miniconda3/envs/ptd_esr_env/lib/python3.10/site-packages/airflow/configuration.py:755 UserWarning: Config scheduler.max_tis_per_query (value: 512) should NOT be greater than core.parallelism (value: 32). Will now use core.parallelism as the max task instances per query instead of specified value.
/home/zhangs/miniconda3/envs/ptd_esr_env/lib/python3.10/site-packages/airflow/cli/cli_config.py:971 DeprecationWarning: The namespace option in [kubernetes] has been moved to the namespace option in [kubernetes_executor] - the old setting has been used, but please update your config.
[[34m2023-10-27T14:18:28.216+1300[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /home/zhangs/airflow/dags/ptd_airflow.py[0m
[[34m2023-10-27T14:18:28.564+1300[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-10-27T14:18:28.604+1300[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/zhangs/miniconda3/envs/ptd_esr_env/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-10-27T14:18:28.605+1300[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-10-27T14:18:29.115+1300[0m] {[34mdagbag.py:[0m346} ERROR[0m - Failed to import: /home/zhangs/miniconda3/envs/ptd_esr_env/lib/python3.10/site-packages/airflow/example_dags/example_subdag_operator.py[0m
Traceback (most recent call last):
  File "/home/zhangs/miniconda3/envs/ptd_esr_env/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/zhangs/miniconda3/envs/ptd_esr_env/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlite3.OperationalError: no such column: slot_pool.include_deferred

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/zhangs/miniconda3/envs/ptd_esr_env/lib/python3.10/site-packages/airflow/models/dagbag.py", line 342, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/zhangs/miniconda3/envs/ptd_esr_env/lib/python3.10/site-packages/airflow/example_dags/example_subdag_operator.py", line 43, in <module>
    section_1 = SubDagOperator(
  File "/home/zhangs/miniconda3/envs/ptd_esr_env/lib/python3.10/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/zhangs/miniconda3/envs/ptd_esr_env/lib/python3.10/site-packages/airflow/utils/session.py", line 77, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/zhangs/miniconda3/envs/ptd_esr_env/lib/python3.10/site-packages/airflow/operators/subdag.py", line 90, in __init__
    self._validate_pool(session)
  File "/home/zhangs/miniconda3/envs/ptd_esr_env/lib/python3.10/site-packages/airflow/operators/subdag.py", line 115, in _validate_pool
    pool = session.query(Pool).filter(Pool.slots == 1).filter(Pool.pool == self.pool).first()
  File "/home/zhangs/miniconda3/envs/ptd_esr_env/lib/python3.10/site-packages/sqlalchemy/orm/query.py", line 2824, in first
    return self.limit(1)._iter().first()
  File "/home/zhangs/miniconda3/envs/ptd_esr_env/lib/python3.10/site-packages/sqlalchemy/orm/query.py", line 2916, in _iter
    result = self.session.execute(
  File "/home/zhangs/miniconda3/envs/ptd_esr_env/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/home/zhangs/miniconda3/envs/ptd_esr_env/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/home/zhangs/miniconda3/envs/ptd_esr_env/lib/python3.10/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/zhangs/miniconda3/envs/ptd_esr_env/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/zhangs/miniconda3/envs/ptd_esr_env/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1953, in _execute_context
    self._handle_dbapi_exception(
  File "/home/zhangs/miniconda3/envs/ptd_esr_env/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2134, in _handle_dbapi_exception
    util.raise_(
  File "/home/zhangs/miniconda3/envs/ptd_esr_env/lib/python3.10/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/zhangs/miniconda3/envs/ptd_esr_env/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/zhangs/miniconda3/envs/ptd_esr_env/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.OperationalError: (sqlite3.OperationalError) no such column: slot_pool.include_deferred
[SQL: SELECT slot_pool.id AS slot_pool_id, slot_pool.pool AS slot_pool_pool, slot_pool.slots AS slot_pool_slots, slot_pool.description AS slot_pool_description, slot_pool.include_deferred AS slot_pool_include_deferred 
FROM slot_pool 
WHERE slot_pool.slots = ? AND slot_pool.pool = ?
 LIMIT ? OFFSET ?]
[parameters: (1, 'default_pool', 1, 0)]
(Background on this error at: https://sqlalche.me/e/14/e3q8)
Traceback (most recent call last):
  File "/home/zhangs/miniconda3/envs/ptd_esr_env/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/zhangs/miniconda3/envs/ptd_esr_env/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlite3.OperationalError: no such column: dag_run.updated_at

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/zhangs/miniconda3/envs/ptd_esr_env/bin/airflow", line 11, in <module>
    sys.exit(main())
  File "/home/zhangs/miniconda3/envs/ptd_esr_env/lib/python3.10/site-packages/airflow/__main__.py", line 59, in main
    args.func(args)
  File "/home/zhangs/miniconda3/envs/ptd_esr_env/lib/python3.10/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/zhangs/miniconda3/envs/ptd_esr_env/lib/python3.10/site-packages/airflow/utils/cli.py", line 113, in wrapper
    return f(*args, **kwargs)
  File "/home/zhangs/miniconda3/envs/ptd_esr_env/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 411, in task_run
    ti, _ = _get_ti(task, args.map_index, exec_date_or_run_id=args.execution_date_or_run_id, pool=args.pool)
  File "/home/zhangs/miniconda3/envs/ptd_esr_env/lib/python3.10/site-packages/airflow/utils/session.py", line 77, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/zhangs/miniconda3/envs/ptd_esr_env/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 175, in _get_ti
    dag_run, dr_created = _get_dag_run(
  File "/home/zhangs/miniconda3/envs/ptd_esr_env/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 111, in _get_dag_run
    dag_run = dag.get_dagrun(run_id=exec_date_or_run_id, session=session)
  File "/home/zhangs/miniconda3/envs/ptd_esr_env/lib/python3.10/site-packages/airflow/utils/session.py", line 74, in wrapper
    return func(*args, **kwargs)
  File "/home/zhangs/miniconda3/envs/ptd_esr_env/lib/python3.10/site-packages/airflow/models/dag.py", line 1491, in get_dagrun
    return session.scalar(query)
  File "/home/zhangs/miniconda3/envs/ptd_esr_env/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
  File "/home/zhangs/miniconda3/envs/ptd_esr_env/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/home/zhangs/miniconda3/envs/ptd_esr_env/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/home/zhangs/miniconda3/envs/ptd_esr_env/lib/python3.10/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/zhangs/miniconda3/envs/ptd_esr_env/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/zhangs/miniconda3/envs/ptd_esr_env/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1953, in _execute_context
    self._handle_dbapi_exception(
  File "/home/zhangs/miniconda3/envs/ptd_esr_env/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2134, in _handle_dbapi_exception
    util.raise_(
  File "/home/zhangs/miniconda3/envs/ptd_esr_env/lib/python3.10/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/zhangs/miniconda3/envs/ptd_esr_env/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/zhangs/miniconda3/envs/ptd_esr_env/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.OperationalError: (sqlite3.OperationalError) no such column: dag_run.updated_at
[SQL: SELECT dag_run.state, dag_run.id, dag_run.dag_id, dag_run.queued_at, dag_run.execution_date, dag_run.start_date, dag_run.end_date, dag_run.run_id, dag_run.creating_job_id, dag_run.external_trigger, dag_run.run_type, dag_run.conf, dag_run.data_interval_start, dag_run.data_interval_end, dag_run.last_scheduling_decision, dag_run.dag_hash, dag_run.log_template_id, dag_run.updated_at 
FROM dag_run 
WHERE dag_run.dag_id = ? AND dag_run.run_id = ?]
[parameters: ('a_call_ptd', 'manual__2023-10-27T01:18:24.970010+00:00')]
(Background on this error at: https://sqlalche.me/e/14/e3q8)
[[34m2023-10-27 14:18:30,314[0m] {[34msequential_executor.py:[0m68} ERROR[0m - Failed to execute task Command '['airflow', 'tasks', 'run', 'a_call_ptd', 'test_ptd', 'manual__2023-10-27T01:18:24.970010+00:00', '--local', '--subdir', 'DAGS_FOLDER/ptd_airflow.py']' returned non-zero exit status 1..[0m
[[34m2023-10-27 14:18:30,315[0m] {[34mscheduler_job.py:[0m588} INFO[0m - Executor reports execution of a_call_ptd.test_ptd run_id=manual__2023-10-27T01:18:24.970010+00:00 exited with status failed for try_number 1[0m
[[34m2023-10-27 14:18:30,501[0m] {[34mscheduler_job.py:[0m631} INFO[0m - TaskInstance Finished: dag_id=a_call_ptd, task_id=test_ptd, run_id=manual__2023-10-27T01:18:24.970010+00:00, map_index=-1, run_start_date=None, run_end_date=None, run_duration=None, state=queued, executor_state=failed, try_number=1, max_tries=0, job_id=None, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-10-27 01:18:26.008701+00:00, queued_by_job_id=17, pid=None[0m
[[34m2023-10-27 14:18:30,501[0m] {[34mscheduler_job.py:[0m673} ERROR[0m - Executor reports task instance <TaskInstance: a_call_ptd.test_ptd manual__2023-10-27T01:18:24.970010+00:00 [queued]> finished (failed) although the task says its queued. (Info: None) Was the task killed externally?[0m
[[34m2023-10-27 14:18:30,501[0m] {[34mtaskinstance.py:[0m1853} ERROR[0m - Executor reports task instance <TaskInstance: a_call_ptd.test_ptd manual__2023-10-27T01:18:24.970010+00:00 [queued]> finished (failed) although the task says its queued. (Info: None) Was the task killed externally?[0m
[[34m2023-10-27 14:18:30,644[0m] {[34mtaskinstance.py:[0m1401} INFO[0m - Marking task as FAILED. dag_id=a_call_ptd, task_id=test_ptd, execution_date=20231027T011824, start_date=, end_date=20231027T011830[0m
[[34m2023-10-27 14:18:31,407[0m] {[34mdagrun.py:[0m578} ERROR[0m - Marking run <DagRun a_call_ptd @ 2023-10-27 01:18:24.970010+00:00: manual__2023-10-27T01:18:24.970010+00:00, state:running, queued_at: 2023-10-27 01:18:25.027400+00:00. externally triggered: True> failed[0m
[[34m2023-10-27 14:18:31,408[0m] {[34mdagrun.py:[0m644} INFO[0m - DagRun Finished: dag_id=a_call_ptd, execution_date=2023-10-27 01:18:24.970010+00:00, run_id=manual__2023-10-27T01:18:24.970010+00:00, run_start_date=2023-10-27 01:18:25.621484+00:00, run_end_date=2023-10-27 01:18:31.408600+00:00, run_duration=5.787116, state=failed, external_trigger=True, run_type=manual, data_interval_start=2023-10-26 01:18:24.970010+00:00, data_interval_end=2023-10-27 01:18:24.970010+00:00, dag_hash=ef39cd88a0086a41612ae0027b034d06[0m
[[34m2023-10-27 14:18:31,423[0m] {[34mdag.py:[0m3336} INFO[0m - Setting next_dagrun for a_call_ptd to 2023-10-27T01:18:24.970010+00:00, run_after=2023-10-28T01:18:24.970010+00:00[0m
[[34m2023-10-27 14:22:55,439[0m] {[34mscheduler_job.py:[0m1380} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2023-10-27 14:25:47,442[0m] {[34mscheduler_job.py:[0m346} INFO[0m - 1 tasks up for execution:
	<TaskInstance: a_call_ptd.test_ptd manual__2023-10-27T01:25:45.900560+00:00 [scheduled]>[0m
[[34m2023-10-27 14:25:47,443[0m] {[34mscheduler_job.py:[0m411} INFO[0m - DAG a_call_ptd has 0/16 running and queued tasks[0m
[[34m2023-10-27 14:25:47,445[0m] {[34mscheduler_job.py:[0m497} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: a_call_ptd.test_ptd manual__2023-10-27T01:25:45.900560+00:00 [scheduled]>[0m
[[34m2023-10-27 14:25:47,454[0m] {[34mscheduler_job.py:[0m536} INFO[0m - Sending TaskInstanceKey(dag_id='a_call_ptd', task_id='test_ptd', run_id='manual__2023-10-27T01:25:45.900560+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-10-27 14:25:47,454[0m] {[34mbase_executor.py:[0m95} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'a_call_ptd', 'test_ptd', 'manual__2023-10-27T01:25:45.900560+00:00', '--local', '--subdir', 'DAGS_FOLDER/ptd_airflow.py'][0m
[[34m2023-10-27 14:25:47,466[0m] {[34msequential_executor.py:[0m61} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'a_call_ptd', 'test_ptd', 'manual__2023-10-27T01:25:45.900560+00:00', '--local', '--subdir', 'DAGS_FOLDER/ptd_airflow.py'][0m
/home/zhangs/miniconda3/envs/ptd_esr_env/lib/python3.10/site-packages/airflow/configuration.py:755 UserWarning: Config scheduler.max_tis_per_query (value: 512) should NOT be greater than core.parallelism (value: 32). Will now use core.parallelism as the max task instances per query instead of specified value.
/home/zhangs/miniconda3/envs/ptd_esr_env/lib/python3.10/site-packages/airflow/cli/cli_config.py:971 DeprecationWarning: The namespace option in [kubernetes] has been moved to the namespace option in [kubernetes_executor] - the old setting has been used, but please update your config.
[[34m2023-10-27T14:25:49.585+1300[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /home/zhangs/airflow/dags/ptd_airflow.py[0m
[[34m2023-10-27T14:25:49.913+1300[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-10-27T14:25:49.951+1300[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/zhangs/miniconda3/envs/ptd_esr_env/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-10-27T14:25:49.952+1300[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-10-27T14:25:50.458+1300[0m] {[34mdagbag.py:[0m346} ERROR[0m - Failed to import: /home/zhangs/miniconda3/envs/ptd_esr_env/lib/python3.10/site-packages/airflow/example_dags/example_subdag_operator.py[0m
Traceback (most recent call last):
  File "/home/zhangs/miniconda3/envs/ptd_esr_env/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/zhangs/miniconda3/envs/ptd_esr_env/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlite3.OperationalError: no such column: slot_pool.include_deferred

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/zhangs/miniconda3/envs/ptd_esr_env/lib/python3.10/site-packages/airflow/models/dagbag.py", line 342, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/zhangs/miniconda3/envs/ptd_esr_env/lib/python3.10/site-packages/airflow/example_dags/example_subdag_operator.py", line 43, in <module>
    section_1 = SubDagOperator(
  File "/home/zhangs/miniconda3/envs/ptd_esr_env/lib/python3.10/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/zhangs/miniconda3/envs/ptd_esr_env/lib/python3.10/site-packages/airflow/utils/session.py", line 77, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/zhangs/miniconda3/envs/ptd_esr_env/lib/python3.10/site-packages/airflow/operators/subdag.py", line 90, in __init__
    self._validate_pool(session)
  File "/home/zhangs/miniconda3/envs/ptd_esr_env/lib/python3.10/site-packages/airflow/operators/subdag.py", line 115, in _validate_pool
    pool = session.query(Pool).filter(Pool.slots == 1).filter(Pool.pool == self.pool).first()
  File "/home/zhangs/miniconda3/envs/ptd_esr_env/lib/python3.10/site-packages/sqlalchemy/orm/query.py", line 2824, in first
    return self.limit(1)._iter().first()
  File "/home/zhangs/miniconda3/envs/ptd_esr_env/lib/python3.10/site-packages/sqlalchemy/orm/query.py", line 2916, in _iter
    result = self.session.execute(
  File "/home/zhangs/miniconda3/envs/ptd_esr_env/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/home/zhangs/miniconda3/envs/ptd_esr_env/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/home/zhangs/miniconda3/envs/ptd_esr_env/lib/python3.10/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/zhangs/miniconda3/envs/ptd_esr_env/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/zhangs/miniconda3/envs/ptd_esr_env/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1953, in _execute_context
    self._handle_dbapi_exception(
  File "/home/zhangs/miniconda3/envs/ptd_esr_env/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2134, in _handle_dbapi_exception
    util.raise_(
  File "/home/zhangs/miniconda3/envs/ptd_esr_env/lib/python3.10/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/zhangs/miniconda3/envs/ptd_esr_env/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/zhangs/miniconda3/envs/ptd_esr_env/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.OperationalError: (sqlite3.OperationalError) no such column: slot_pool.include_deferred
[SQL: SELECT slot_pool.id AS slot_pool_id, slot_pool.pool AS slot_pool_pool, slot_pool.slots AS slot_pool_slots, slot_pool.description AS slot_pool_description, slot_pool.include_deferred AS slot_pool_include_deferred 
FROM slot_pool 
WHERE slot_pool.slots = ? AND slot_pool.pool = ?
 LIMIT ? OFFSET ?]
[parameters: (1, 'default_pool', 1, 0)]
(Background on this error at: https://sqlalche.me/e/14/e3q8)
Traceback (most recent call last):
  File "/home/zhangs/miniconda3/envs/ptd_esr_env/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/zhangs/miniconda3/envs/ptd_esr_env/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlite3.OperationalError: no such column: dag_run.updated_at

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/zhangs/miniconda3/envs/ptd_esr_env/bin/airflow", line 11, in <module>
    sys.exit(main())
  File "/home/zhangs/miniconda3/envs/ptd_esr_env/lib/python3.10/site-packages/airflow/__main__.py", line 59, in main
    args.func(args)
  File "/home/zhangs/miniconda3/envs/ptd_esr_env/lib/python3.10/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/zhangs/miniconda3/envs/ptd_esr_env/lib/python3.10/site-packages/airflow/utils/cli.py", line 113, in wrapper
    return f(*args, **kwargs)
  File "/home/zhangs/miniconda3/envs/ptd_esr_env/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 411, in task_run
    ti, _ = _get_ti(task, args.map_index, exec_date_or_run_id=args.execution_date_or_run_id, pool=args.pool)
  File "/home/zhangs/miniconda3/envs/ptd_esr_env/lib/python3.10/site-packages/airflow/utils/session.py", line 77, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/zhangs/miniconda3/envs/ptd_esr_env/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 175, in _get_ti
    dag_run, dr_created = _get_dag_run(
  File "/home/zhangs/miniconda3/envs/ptd_esr_env/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 111, in _get_dag_run
    dag_run = dag.get_dagrun(run_id=exec_date_or_run_id, session=session)
  File "/home/zhangs/miniconda3/envs/ptd_esr_env/lib/python3.10/site-packages/airflow/utils/session.py", line 74, in wrapper
    return func(*args, **kwargs)
  File "/home/zhangs/miniconda3/envs/ptd_esr_env/lib/python3.10/site-packages/airflow/models/dag.py", line 1491, in get_dagrun
    return session.scalar(query)
  File "/home/zhangs/miniconda3/envs/ptd_esr_env/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
  File "/home/zhangs/miniconda3/envs/ptd_esr_env/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/home/zhangs/miniconda3/envs/ptd_esr_env/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/home/zhangs/miniconda3/envs/ptd_esr_env/lib/python3.10/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/zhangs/miniconda3/envs/ptd_esr_env/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/zhangs/miniconda3/envs/ptd_esr_env/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1953, in _execute_context
    self._handle_dbapi_exception(
  File "/home/zhangs/miniconda3/envs/ptd_esr_env/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2134, in _handle_dbapi_exception
    util.raise_(
  File "/home/zhangs/miniconda3/envs/ptd_esr_env/lib/python3.10/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/zhangs/miniconda3/envs/ptd_esr_env/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/zhangs/miniconda3/envs/ptd_esr_env/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.OperationalError: (sqlite3.OperationalError) no such column: dag_run.updated_at
[SQL: SELECT dag_run.state, dag_run.id, dag_run.dag_id, dag_run.queued_at, dag_run.execution_date, dag_run.start_date, dag_run.end_date, dag_run.run_id, dag_run.creating_job_id, dag_run.external_trigger, dag_run.run_type, dag_run.conf, dag_run.data_interval_start, dag_run.data_interval_end, dag_run.last_scheduling_decision, dag_run.dag_hash, dag_run.log_template_id, dag_run.updated_at 
FROM dag_run 
WHERE dag_run.dag_id = ? AND dag_run.run_id = ?]
[parameters: ('a_call_ptd', 'manual__2023-10-27T01:25:45.900560+00:00')]
(Background on this error at: https://sqlalche.me/e/14/e3q8)
[[34m2023-10-27 14:25:51,532[0m] {[34msequential_executor.py:[0m68} ERROR[0m - Failed to execute task Command '['airflow', 'tasks', 'run', 'a_call_ptd', 'test_ptd', 'manual__2023-10-27T01:25:45.900560+00:00', '--local', '--subdir', 'DAGS_FOLDER/ptd_airflow.py']' returned non-zero exit status 1..[0m
[[34m2023-10-27 14:25:51,534[0m] {[34mscheduler_job.py:[0m588} INFO[0m - Executor reports execution of a_call_ptd.test_ptd run_id=manual__2023-10-27T01:25:45.900560+00:00 exited with status failed for try_number 1[0m
[[34m2023-10-27 14:25:51,563[0m] {[34mscheduler_job.py:[0m631} INFO[0m - TaskInstance Finished: dag_id=a_call_ptd, task_id=test_ptd, run_id=manual__2023-10-27T01:25:45.900560+00:00, map_index=-1, run_start_date=None, run_end_date=None, run_duration=None, state=queued, executor_state=failed, try_number=1, max_tries=0, job_id=None, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-10-27 01:25:47.446452+00:00, queued_by_job_id=17, pid=None[0m
[[34m2023-10-27 14:25:51,563[0m] {[34mscheduler_job.py:[0m673} ERROR[0m - Executor reports task instance <TaskInstance: a_call_ptd.test_ptd manual__2023-10-27T01:25:45.900560+00:00 [queued]> finished (failed) although the task says its queued. (Info: None) Was the task killed externally?[0m
[[34m2023-10-27 14:25:51,570[0m] {[34mtaskinstance.py:[0m1853} ERROR[0m - Executor reports task instance <TaskInstance: a_call_ptd.test_ptd manual__2023-10-27T01:25:45.900560+00:00 [queued]> finished (failed) although the task says its queued. (Info: None) Was the task killed externally?[0m
[[34m2023-10-27 14:25:51,596[0m] {[34mtaskinstance.py:[0m1401} INFO[0m - Marking task as FAILED. dag_id=a_call_ptd, task_id=test_ptd, execution_date=20231027T012545, start_date=, end_date=20231027T012551[0m
[[34m2023-10-27 14:25:51,953[0m] {[34mdagrun.py:[0m578} ERROR[0m - Marking run <DagRun a_call_ptd @ 2023-10-27 01:25:45.900560+00:00: manual__2023-10-27T01:25:45.900560+00:00, state:running, queued_at: 2023-10-27 01:25:45.959464+00:00. externally triggered: True> failed[0m
[[34m2023-10-27 14:25:51,954[0m] {[34mdagrun.py:[0m644} INFO[0m - DagRun Finished: dag_id=a_call_ptd, execution_date=2023-10-27 01:25:45.900560+00:00, run_id=manual__2023-10-27T01:25:45.900560+00:00, run_start_date=2023-10-27 01:25:47.301296+00:00, run_end_date=2023-10-27 01:25:51.953998+00:00, run_duration=4.652702, state=failed, external_trigger=True, run_type=manual, data_interval_start=2023-10-26 01:25:45.900560+00:00, data_interval_end=2023-10-27 01:25:45.900560+00:00, dag_hash=ef39cd88a0086a41612ae0027b034d06[0m
[[34m2023-10-27 14:25:51,963[0m] {[34mdag.py:[0m3336} INFO[0m - Setting next_dagrun for a_call_ptd to 2023-10-27T01:25:45.900560+00:00, run_after=2023-10-28T01:25:45.900560+00:00[0m
[[34m2023-10-27 14:27:25,977[0m] {[34mdag.py:[0m3336} INFO[0m - Setting next_dagrun for a_call_ptd to 2023-10-27T00:00:00+00:00, run_after=2023-10-28T00:00:00+00:00[0m
[[34m2023-10-27 14:27:26,245[0m] {[34mscheduler_job.py:[0m346} INFO[0m - 1 tasks up for execution:
	<TaskInstance: a_call_ptd.test_ptd scheduled__2023-10-26T00:00:00+00:00 [scheduled]>[0m
[[34m2023-10-27 14:27:26,245[0m] {[34mscheduler_job.py:[0m411} INFO[0m - DAG a_call_ptd has 0/16 running and queued tasks[0m
[[34m2023-10-27 14:27:26,245[0m] {[34mscheduler_job.py:[0m497} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: a_call_ptd.test_ptd scheduled__2023-10-26T00:00:00+00:00 [scheduled]>[0m
[[34m2023-10-27 14:27:26,383[0m] {[34mscheduler_job.py:[0m536} INFO[0m - Sending TaskInstanceKey(dag_id='a_call_ptd', task_id='test_ptd', run_id='scheduled__2023-10-26T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-10-27 14:27:26,384[0m] {[34mbase_executor.py:[0m95} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'a_call_ptd', 'test_ptd', 'scheduled__2023-10-26T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/ptd_airflow.py'][0m
[[34m2023-10-27 14:27:26,398[0m] {[34msequential_executor.py:[0m61} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'a_call_ptd', 'test_ptd', 'scheduled__2023-10-26T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/ptd_airflow.py'][0m
/home/zhangs/miniconda3/envs/ptd_esr_env/lib/python3.10/site-packages/airflow/configuration.py:755 UserWarning: Config scheduler.max_tis_per_query (value: 512) should NOT be greater than core.parallelism (value: 32). Will now use core.parallelism as the max task instances per query instead of specified value.
/home/zhangs/miniconda3/envs/ptd_esr_env/lib/python3.10/site-packages/airflow/cli/cli_config.py:971 DeprecationWarning: The namespace option in [kubernetes] has been moved to the namespace option in [kubernetes_executor] - the old setting has been used, but please update your config.
[[34m2023-10-27T14:27:28.548+1300[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /home/zhangs/airflow/dags/ptd_airflow.py[0m
[[34m2023-10-27T14:27:28.897+1300[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-10-27T14:27:28.937+1300[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/zhangs/miniconda3/envs/ptd_esr_env/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-10-27T14:27:28.938+1300[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-10-27T14:27:29.476+1300[0m] {[34mdagbag.py:[0m346} ERROR[0m - Failed to import: /home/zhangs/miniconda3/envs/ptd_esr_env/lib/python3.10/site-packages/airflow/example_dags/example_subdag_operator.py[0m
Traceback (most recent call last):
  File "/home/zhangs/miniconda3/envs/ptd_esr_env/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/zhangs/miniconda3/envs/ptd_esr_env/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlite3.OperationalError: no such column: slot_pool.include_deferred

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/zhangs/miniconda3/envs/ptd_esr_env/lib/python3.10/site-packages/airflow/models/dagbag.py", line 342, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/zhangs/miniconda3/envs/ptd_esr_env/lib/python3.10/site-packages/airflow/example_dags/example_subdag_operator.py", line 43, in <module>
    section_1 = SubDagOperator(
  File "/home/zhangs/miniconda3/envs/ptd_esr_env/lib/python3.10/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/zhangs/miniconda3/envs/ptd_esr_env/lib/python3.10/site-packages/airflow/utils/session.py", line 77, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/zhangs/miniconda3/envs/ptd_esr_env/lib/python3.10/site-packages/airflow/operators/subdag.py", line 90, in __init__
    self._validate_pool(session)
  File "/home/zhangs/miniconda3/envs/ptd_esr_env/lib/python3.10/site-packages/airflow/operators/subdag.py", line 115, in _validate_pool
    pool = session.query(Pool).filter(Pool.slots == 1).filter(Pool.pool == self.pool).first()
  File "/home/zhangs/miniconda3/envs/ptd_esr_env/lib/python3.10/site-packages/sqlalchemy/orm/query.py", line 2824, in first
    return self.limit(1)._iter().first()
  File "/home/zhangs/miniconda3/envs/ptd_esr_env/lib/python3.10/site-packages/sqlalchemy/orm/query.py", line 2916, in _iter
    result = self.session.execute(
  File "/home/zhangs/miniconda3/envs/ptd_esr_env/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/home/zhangs/miniconda3/envs/ptd_esr_env/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/home/zhangs/miniconda3/envs/ptd_esr_env/lib/python3.10/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/zhangs/miniconda3/envs/ptd_esr_env/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/zhangs/miniconda3/envs/ptd_esr_env/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1953, in _execute_context
    self._handle_dbapi_exception(
  File "/home/zhangs/miniconda3/envs/ptd_esr_env/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2134, in _handle_dbapi_exception
    util.raise_(
  File "/home/zhangs/miniconda3/envs/ptd_esr_env/lib/python3.10/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/zhangs/miniconda3/envs/ptd_esr_env/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/zhangs/miniconda3/envs/ptd_esr_env/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.OperationalError: (sqlite3.OperationalError) no such column: slot_pool.include_deferred
[SQL: SELECT slot_pool.id AS slot_pool_id, slot_pool.pool AS slot_pool_pool, slot_pool.slots AS slot_pool_slots, slot_pool.description AS slot_pool_description, slot_pool.include_deferred AS slot_pool_include_deferred 
FROM slot_pool 
WHERE slot_pool.slots = ? AND slot_pool.pool = ?
 LIMIT ? OFFSET ?]
[parameters: (1, 'default_pool', 1, 0)]
(Background on this error at: https://sqlalche.me/e/14/e3q8)
Traceback (most recent call last):
  File "/home/zhangs/miniconda3/envs/ptd_esr_env/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/zhangs/miniconda3/envs/ptd_esr_env/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlite3.OperationalError: no such column: dag_run.updated_at

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/zhangs/miniconda3/envs/ptd_esr_env/bin/airflow", line 11, in <module>
    sys.exit(main())
  File "/home/zhangs/miniconda3/envs/ptd_esr_env/lib/python3.10/site-packages/airflow/__main__.py", line 59, in main
    args.func(args)
  File "/home/zhangs/miniconda3/envs/ptd_esr_env/lib/python3.10/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/zhangs/miniconda3/envs/ptd_esr_env/lib/python3.10/site-packages/airflow/utils/cli.py", line 113, in wrapper
    return f(*args, **kwargs)
  File "/home/zhangs/miniconda3/envs/ptd_esr_env/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 411, in task_run
    ti, _ = _get_ti(task, args.map_index, exec_date_or_run_id=args.execution_date_or_run_id, pool=args.pool)
  File "/home/zhangs/miniconda3/envs/ptd_esr_env/lib/python3.10/site-packages/airflow/utils/session.py", line 77, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/zhangs/miniconda3/envs/ptd_esr_env/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 175, in _get_ti
    dag_run, dr_created = _get_dag_run(
  File "/home/zhangs/miniconda3/envs/ptd_esr_env/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 111, in _get_dag_run
    dag_run = dag.get_dagrun(run_id=exec_date_or_run_id, session=session)
  File "/home/zhangs/miniconda3/envs/ptd_esr_env/lib/python3.10/site-packages/airflow/utils/session.py", line 74, in wrapper
    return func(*args, **kwargs)
  File "/home/zhangs/miniconda3/envs/ptd_esr_env/lib/python3.10/site-packages/airflow/models/dag.py", line 1491, in get_dagrun
    return session.scalar(query)
  File "/home/zhangs/miniconda3/envs/ptd_esr_env/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
  File "/home/zhangs/miniconda3/envs/ptd_esr_env/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/home/zhangs/miniconda3/envs/ptd_esr_env/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/home/zhangs/miniconda3/envs/ptd_esr_env/lib/python3.10/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/zhangs/miniconda3/envs/ptd_esr_env/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/zhangs/miniconda3/envs/ptd_esr_env/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1953, in _execute_context
    self._handle_dbapi_exception(
  File "/home/zhangs/miniconda3/envs/ptd_esr_env/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2134, in _handle_dbapi_exception
    util.raise_(
  File "/home/zhangs/miniconda3/envs/ptd_esr_env/lib/python3.10/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/zhangs/miniconda3/envs/ptd_esr_env/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/zhangs/miniconda3/envs/ptd_esr_env/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.OperationalError: (sqlite3.OperationalError) no such column: dag_run.updated_at
[SQL: SELECT dag_run.state, dag_run.id, dag_run.dag_id, dag_run.queued_at, dag_run.execution_date, dag_run.start_date, dag_run.end_date, dag_run.run_id, dag_run.creating_job_id, dag_run.external_trigger, dag_run.run_type, dag_run.conf, dag_run.data_interval_start, dag_run.data_interval_end, dag_run.last_scheduling_decision, dag_run.dag_hash, dag_run.log_template_id, dag_run.updated_at 
FROM dag_run 
WHERE dag_run.dag_id = ? AND dag_run.run_id = ?]
[parameters: ('a_call_ptd', 'scheduled__2023-10-26T00:00:00+00:00')]
(Background on this error at: https://sqlalche.me/e/14/e3q8)
[[34m2023-10-27 14:27:30,732[0m] {[34msequential_executor.py:[0m68} ERROR[0m - Failed to execute task Command '['airflow', 'tasks', 'run', 'a_call_ptd', 'test_ptd', 'scheduled__2023-10-26T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/ptd_airflow.py']' returned non-zero exit status 1..[0m
[[34m2023-10-27 14:27:30,734[0m] {[34mscheduler_job.py:[0m588} INFO[0m - Executor reports execution of a_call_ptd.test_ptd run_id=scheduled__2023-10-26T00:00:00+00:00 exited with status failed for try_number 1[0m
[[34m2023-10-27 14:27:30,767[0m] {[34mscheduler_job.py:[0m631} INFO[0m - TaskInstance Finished: dag_id=a_call_ptd, task_id=test_ptd, run_id=scheduled__2023-10-26T00:00:00+00:00, map_index=-1, run_start_date=None, run_end_date=None, run_duration=None, state=queued, executor_state=failed, try_number=1, max_tries=0, job_id=None, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-10-27 01:27:26.247156+00:00, queued_by_job_id=17, pid=None[0m
[[34m2023-10-27 14:27:30,768[0m] {[34mscheduler_job.py:[0m673} ERROR[0m - Executor reports task instance <TaskInstance: a_call_ptd.test_ptd scheduled__2023-10-26T00:00:00+00:00 [queued]> finished (failed) although the task says its queued. (Info: None) Was the task killed externally?[0m
[[34m2023-10-27 14:27:30,774[0m] {[34mtaskinstance.py:[0m1853} ERROR[0m - Executor reports task instance <TaskInstance: a_call_ptd.test_ptd scheduled__2023-10-26T00:00:00+00:00 [queued]> finished (failed) although the task says its queued. (Info: None) Was the task killed externally?[0m
[[34m2023-10-27 14:27:30,802[0m] {[34mtaskinstance.py:[0m1401} INFO[0m - Marking task as FAILED. dag_id=a_call_ptd, task_id=test_ptd, execution_date=20231026T000000, start_date=, end_date=20231027T012730[0m
[[34m2023-10-27 14:27:31,558[0m] {[34mdagrun.py:[0m578} ERROR[0m - Marking run <DagRun a_call_ptd @ 2023-10-26 00:00:00+00:00: scheduled__2023-10-26T00:00:00+00:00, state:running, queued_at: 2023-10-27 01:27:25.955665+00:00. externally triggered: False> failed[0m
[[34m2023-10-27 14:27:31,558[0m] {[34mdagrun.py:[0m644} INFO[0m - DagRun Finished: dag_id=a_call_ptd, execution_date=2023-10-26 00:00:00+00:00, run_id=scheduled__2023-10-26T00:00:00+00:00, run_start_date=2023-10-27 01:27:26.044196+00:00, run_end_date=2023-10-27 01:27:31.558511+00:00, run_duration=5.514315, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2023-10-26 00:00:00+00:00, data_interval_end=2023-10-27 00:00:00+00:00, dag_hash=ef39cd88a0086a41612ae0027b034d06[0m
[[34m2023-10-27 14:27:31,565[0m] {[34mdagrun.py:[0m837} WARNING[0m - Failed to record first_task_scheduling_delay metric:[0m
Traceback (most recent call last):
  File "/home/zhangs/miniconda3/envs/airflow/lib/python3.9/site-packages/airflow/models/dagrun.py", line 825, in _emit_true_scheduling_delay_stats_for_finished_state
    first_start_date = ordered_tis_by_start_date[0].start_date
IndexError: list index out of range
[[34m2023-10-27 14:27:31,570[0m] {[34mdag.py:[0m3336} INFO[0m - Setting next_dagrun for a_call_ptd to 2023-10-27T00:00:00+00:00, run_after=2023-10-28T00:00:00+00:00[0m
[[34m2023-10-27 14:27:56,290[0m] {[34mscheduler_job.py:[0m1380} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2023-10-27 14:32:56,920[0m] {[34mscheduler_job.py:[0m1380} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2023-10-27 14:35:10,135[0m] {[34mscheduler_job.py:[0m346} INFO[0m - 1 tasks up for execution:
	<TaskInstance: hello_world.hello_world_task manual__2023-10-27T01:35:09.812739+00:00 [scheduled]>[0m
[[34m2023-10-27 14:35:10,136[0m] {[34mscheduler_job.py:[0m411} INFO[0m - DAG hello_world has 0/16 running and queued tasks[0m
[[34m2023-10-27 14:35:10,137[0m] {[34mscheduler_job.py:[0m497} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: hello_world.hello_world_task manual__2023-10-27T01:35:09.812739+00:00 [scheduled]>[0m
[[34m2023-10-27 14:35:10,147[0m] {[34mscheduler_job.py:[0m536} INFO[0m - Sending TaskInstanceKey(dag_id='hello_world', task_id='hello_world_task', run_id='manual__2023-10-27T01:35:09.812739+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-10-27 14:35:10,147[0m] {[34mbase_executor.py:[0m95} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'hello_world', 'hello_world_task', 'manual__2023-10-27T01:35:09.812739+00:00', '--local', '--subdir', 'DAGS_FOLDER/ptd_airflow.py'][0m
[[34m2023-10-27 14:35:10,158[0m] {[34msequential_executor.py:[0m61} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'hello_world', 'hello_world_task', 'manual__2023-10-27T01:35:09.812739+00:00', '--local', '--subdir', 'DAGS_FOLDER/ptd_airflow.py'][0m
/home/zhangs/miniconda3/envs/ptd_esr_env/lib/python3.10/site-packages/airflow/configuration.py:755 UserWarning: Config scheduler.max_tis_per_query (value: 512) should NOT be greater than core.parallelism (value: 32). Will now use core.parallelism as the max task instances per query instead of specified value.
/home/zhangs/miniconda3/envs/ptd_esr_env/lib/python3.10/site-packages/airflow/cli/cli_config.py:971 DeprecationWarning: The namespace option in [kubernetes] has been moved to the namespace option in [kubernetes_executor] - the old setting has been used, but please update your config.
[[34m2023-10-27T14:35:12.282+1300[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /home/zhangs/airflow/dags/ptd_airflow.py[0m
[[34m2023-10-27T14:35:12.624+1300[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-10-27T14:35:12.672+1300[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/zhangs/miniconda3/envs/ptd_esr_env/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-10-27T14:35:12.672+1300[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-10-27T14:35:13.188+1300[0m] {[34mdagbag.py:[0m346} ERROR[0m - Failed to import: /home/zhangs/miniconda3/envs/ptd_esr_env/lib/python3.10/site-packages/airflow/example_dags/example_subdag_operator.py[0m
Traceback (most recent call last):
  File "/home/zhangs/miniconda3/envs/ptd_esr_env/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/zhangs/miniconda3/envs/ptd_esr_env/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlite3.OperationalError: no such column: slot_pool.include_deferred

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/zhangs/miniconda3/envs/ptd_esr_env/lib/python3.10/site-packages/airflow/models/dagbag.py", line 342, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/zhangs/miniconda3/envs/ptd_esr_env/lib/python3.10/site-packages/airflow/example_dags/example_subdag_operator.py", line 43, in <module>
    section_1 = SubDagOperator(
  File "/home/zhangs/miniconda3/envs/ptd_esr_env/lib/python3.10/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/zhangs/miniconda3/envs/ptd_esr_env/lib/python3.10/site-packages/airflow/utils/session.py", line 77, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/zhangs/miniconda3/envs/ptd_esr_env/lib/python3.10/site-packages/airflow/operators/subdag.py", line 90, in __init__
    self._validate_pool(session)
  File "/home/zhangs/miniconda3/envs/ptd_esr_env/lib/python3.10/site-packages/airflow/operators/subdag.py", line 115, in _validate_pool
    pool = session.query(Pool).filter(Pool.slots == 1).filter(Pool.pool == self.pool).first()
  File "/home/zhangs/miniconda3/envs/ptd_esr_env/lib/python3.10/site-packages/sqlalchemy/orm/query.py", line 2824, in first
    return self.limit(1)._iter().first()
  File "/home/zhangs/miniconda3/envs/ptd_esr_env/lib/python3.10/site-packages/sqlalchemy/orm/query.py", line 2916, in _iter
    result = self.session.execute(
  File "/home/zhangs/miniconda3/envs/ptd_esr_env/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/home/zhangs/miniconda3/envs/ptd_esr_env/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/home/zhangs/miniconda3/envs/ptd_esr_env/lib/python3.10/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/zhangs/miniconda3/envs/ptd_esr_env/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/zhangs/miniconda3/envs/ptd_esr_env/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1953, in _execute_context
    self._handle_dbapi_exception(
  File "/home/zhangs/miniconda3/envs/ptd_esr_env/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2134, in _handle_dbapi_exception
    util.raise_(
  File "/home/zhangs/miniconda3/envs/ptd_esr_env/lib/python3.10/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/zhangs/miniconda3/envs/ptd_esr_env/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/zhangs/miniconda3/envs/ptd_esr_env/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.OperationalError: (sqlite3.OperationalError) no such column: slot_pool.include_deferred
[SQL: SELECT slot_pool.id AS slot_pool_id, slot_pool.pool AS slot_pool_pool, slot_pool.slots AS slot_pool_slots, slot_pool.description AS slot_pool_description, slot_pool.include_deferred AS slot_pool_include_deferred 
FROM slot_pool 
WHERE slot_pool.slots = ? AND slot_pool.pool = ?
 LIMIT ? OFFSET ?]
[parameters: (1, 'default_pool', 1, 0)]
(Background on this error at: https://sqlalche.me/e/14/e3q8)
Traceback (most recent call last):
  File "/home/zhangs/miniconda3/envs/ptd_esr_env/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/zhangs/miniconda3/envs/ptd_esr_env/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlite3.OperationalError: no such column: dag_run.updated_at

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/zhangs/miniconda3/envs/ptd_esr_env/bin/airflow", line 11, in <module>
    sys.exit(main())
  File "/home/zhangs/miniconda3/envs/ptd_esr_env/lib/python3.10/site-packages/airflow/__main__.py", line 59, in main
    args.func(args)
  File "/home/zhangs/miniconda3/envs/ptd_esr_env/lib/python3.10/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/zhangs/miniconda3/envs/ptd_esr_env/lib/python3.10/site-packages/airflow/utils/cli.py", line 113, in wrapper
    return f(*args, **kwargs)
  File "/home/zhangs/miniconda3/envs/ptd_esr_env/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 411, in task_run
    ti, _ = _get_ti(task, args.map_index, exec_date_or_run_id=args.execution_date_or_run_id, pool=args.pool)
  File "/home/zhangs/miniconda3/envs/ptd_esr_env/lib/python3.10/site-packages/airflow/utils/session.py", line 77, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/zhangs/miniconda3/envs/ptd_esr_env/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 175, in _get_ti
    dag_run, dr_created = _get_dag_run(
  File "/home/zhangs/miniconda3/envs/ptd_esr_env/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 111, in _get_dag_run
    dag_run = dag.get_dagrun(run_id=exec_date_or_run_id, session=session)
  File "/home/zhangs/miniconda3/envs/ptd_esr_env/lib/python3.10/site-packages/airflow/utils/session.py", line 74, in wrapper
    return func(*args, **kwargs)
  File "/home/zhangs/miniconda3/envs/ptd_esr_env/lib/python3.10/site-packages/airflow/models/dag.py", line 1491, in get_dagrun
    return session.scalar(query)
  File "/home/zhangs/miniconda3/envs/ptd_esr_env/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
  File "/home/zhangs/miniconda3/envs/ptd_esr_env/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/home/zhangs/miniconda3/envs/ptd_esr_env/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/home/zhangs/miniconda3/envs/ptd_esr_env/lib/python3.10/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/zhangs/miniconda3/envs/ptd_esr_env/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/zhangs/miniconda3/envs/ptd_esr_env/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1953, in _execute_context
    self._handle_dbapi_exception(
  File "/home/zhangs/miniconda3/envs/ptd_esr_env/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2134, in _handle_dbapi_exception
    util.raise_(
  File "/home/zhangs/miniconda3/envs/ptd_esr_env/lib/python3.10/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/zhangs/miniconda3/envs/ptd_esr_env/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/zhangs/miniconda3/envs/ptd_esr_env/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.OperationalError: (sqlite3.OperationalError) no such column: dag_run.updated_at
[SQL: SELECT dag_run.state, dag_run.id, dag_run.dag_id, dag_run.queued_at, dag_run.execution_date, dag_run.start_date, dag_run.end_date, dag_run.run_id, dag_run.creating_job_id, dag_run.external_trigger, dag_run.run_type, dag_run.conf, dag_run.data_interval_start, dag_run.data_interval_end, dag_run.last_scheduling_decision, dag_run.dag_hash, dag_run.log_template_id, dag_run.updated_at 
FROM dag_run 
WHERE dag_run.dag_id = ? AND dag_run.run_id = ?]
[parameters: ('hello_world', 'manual__2023-10-27T01:35:09.812739+00:00')]
(Background on this error at: https://sqlalche.me/e/14/e3q8)
[[34m2023-10-27 14:35:14,396[0m] {[34msequential_executor.py:[0m68} ERROR[0m - Failed to execute task Command '['airflow', 'tasks', 'run', 'hello_world', 'hello_world_task', 'manual__2023-10-27T01:35:09.812739+00:00', '--local', '--subdir', 'DAGS_FOLDER/ptd_airflow.py']' returned non-zero exit status 1..[0m
[[34m2023-10-27 14:35:14,398[0m] {[34mscheduler_job.py:[0m588} INFO[0m - Executor reports execution of hello_world.hello_world_task run_id=manual__2023-10-27T01:35:09.812739+00:00 exited with status failed for try_number 1[0m
[[34m2023-10-27 14:35:14,424[0m] {[34mscheduler_job.py:[0m631} INFO[0m - TaskInstance Finished: dag_id=hello_world, task_id=hello_world_task, run_id=manual__2023-10-27T01:35:09.812739+00:00, map_index=-1, run_start_date=None, run_end_date=None, run_duration=None, state=queued, executor_state=failed, try_number=1, max_tries=0, job_id=None, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-10-27 01:35:10.139214+00:00, queued_by_job_id=17, pid=None[0m
[[34m2023-10-27 14:35:14,425[0m] {[34mscheduler_job.py:[0m673} ERROR[0m - Executor reports task instance <TaskInstance: hello_world.hello_world_task manual__2023-10-27T01:35:09.812739+00:00 [queued]> finished (failed) although the task says its queued. (Info: None) Was the task killed externally?[0m
[[34m2023-10-27 14:35:14,426[0m] {[34mtaskinstance.py:[0m1853} ERROR[0m - Executor reports task instance <TaskInstance: hello_world.hello_world_task manual__2023-10-27T01:35:09.812739+00:00 [queued]> finished (failed) although the task says its queued. (Info: None) Was the task killed externally?[0m
[[34m2023-10-27 14:35:14,450[0m] {[34mtaskinstance.py:[0m1401} INFO[0m - Marking task as FAILED. dag_id=hello_world, task_id=hello_world_task, execution_date=20231027T013509, start_date=, end_date=20231027T013514[0m
[[34m2023-10-27 14:35:14,997[0m] {[34mdagrun.py:[0m578} ERROR[0m - Marking run <DagRun hello_world @ 2023-10-27 01:35:09.812739+00:00: manual__2023-10-27T01:35:09.812739+00:00, state:running, queued_at: 2023-10-27 01:35:09.914494+00:00. externally triggered: True> failed[0m
[[34m2023-10-27 14:35:14,998[0m] {[34mdagrun.py:[0m644} INFO[0m - DagRun Finished: dag_id=hello_world, execution_date=2023-10-27 01:35:09.812739+00:00, run_id=manual__2023-10-27T01:35:09.812739+00:00, run_start_date=2023-10-27 01:35:10.024052+00:00, run_end_date=2023-10-27 01:35:14.998333+00:00, run_duration=4.974281, state=failed, external_trigger=True, run_type=manual, data_interval_start=2023-10-27 01:35:09.812739+00:00, data_interval_end=2023-10-27 01:35:09.812739+00:00, dag_hash=8f854b03cac532842b5b2006c82363e1[0m
[[34m2023-10-27 14:35:15,006[0m] {[34mdag.py:[0m3336} INFO[0m - Setting next_dagrun for hello_world to None, run_after=None[0m
[[34m2023-10-27 14:37:57,541[0m] {[34mscheduler_job.py:[0m1380} INFO[0m - Resetting orphaned tasks for active dag runs[0m
/home/zhangs/miniconda3/envs/airflow/lib/python3.9/site-packages/airflow/models/base.py:49 MovedIn20Warning: Deprecated API features detected! These feature(s) are not compatible with SQLAlchemy 2.0. To prevent incompatible upgrades prior to updating applications, ensure requirements files are pinned to "sqlalchemy<2.0". Set environment variable SQLALCHEMY_WARN_20=1 to show all deprecation warnings.  Set environment variable SQLALCHEMY_SILENCE_UBER_WARNING=1 to silence this message. (Background on SQLAlchemy 2.0 at: https://sqlalche.me/e/b8d9)
  ____________       _____________
 ____    |__( )_________  __/__  /________      __
____  /| |_  /__  ___/_  /_ __  /_  __ \_ | /| / /
___  ___ |  / _  /   _  __/ _  / / /_/ /_ |/ |/ /
 _/_/  |_/_/  /_/    /_/    /_/  \____/____/|__/
[2023-10-27 14:41:23 +1300] [3124574] [INFO] Starting gunicorn 21.2.0
[2023-10-27 14:41:23 +1300] [3124574] [INFO] Listening at: http://[::]:8083 (3124574)
[2023-10-27 14:41:23 +1300] [3124574] [INFO] Using worker: sync
[[34m2023-10-27 14:41:23,254[0m] {[34mscheduler_job.py:[0m700} INFO[0m - Starting the scheduler[0m
[2023-10-27 14:41:23 +1300] [3124597] [INFO] Booting worker with pid: 3124597
[[34m2023-10-27 14:41:23,255[0m] {[34mscheduler_job.py:[0m705} INFO[0m - Processing each file at most -1 times[0m
[[34m2023-10-27 14:41:23,258[0m] {[34mexecutor_loader.py:[0m107} INFO[0m - Loaded executor: SequentialExecutor[0m
[[34m2023-10-27 14:41:23,264[0m] {[34mmanager.py:[0m163} INFO[0m - Launched DagFileProcessorManager with pid: 3124608[0m
[[34m2023-10-27 14:41:23,266[0m] {[34mscheduler_job.py:[0m1380} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2023-10-27 14:41:23,275[0m] {[34msettings.py:[0m58} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2023-10-27 14:41:23 +1300] [3124614] [INFO] Booting worker with pid: 3124614
[2023-10-27T14:41:23.291+1300] {manager.py:409} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2) when using sqlite. So we set parallelism to 1.
[[34m2023-10-27 14:41:23,306[0m] {[34mscheduler_job.py:[0m1403} INFO[0m - Marked 1 SchedulerJob instances as failed[0m
[[34m2023-10-27 14:41:24,089[0m] {[34mscheduler_job.py:[0m346} INFO[0m - 1 tasks up for execution:
	<TaskInstance: hello_world.hello_world_task manual__2023-10-27T01:40:07.188370+00:00 [scheduled]>[0m
[[34m2023-10-27 14:41:24,090[0m] {[34mscheduler_job.py:[0m411} INFO[0m - DAG hello_world has 0/16 running and queued tasks[0m
[[34m2023-10-27 14:41:24,090[0m] {[34mscheduler_job.py:[0m497} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: hello_world.hello_world_task manual__2023-10-27T01:40:07.188370+00:00 [scheduled]>[0m
[[34m2023-10-27 14:41:24,098[0m] {[34mscheduler_job.py:[0m536} INFO[0m - Sending TaskInstanceKey(dag_id='hello_world', task_id='hello_world_task', run_id='manual__2023-10-27T01:40:07.188370+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-10-27 14:41:24,098[0m] {[34mbase_executor.py:[0m95} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'hello_world', 'hello_world_task', 'manual__2023-10-27T01:40:07.188370+00:00', '--local', '--subdir', 'DAGS_FOLDER/ptd_airflow.py'][0m
[[34m2023-10-27 14:41:24,109[0m] {[34msequential_executor.py:[0m61} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'hello_world', 'hello_world_task', 'manual__2023-10-27T01:40:07.188370+00:00', '--local', '--subdir', 'DAGS_FOLDER/ptd_airflow.py'][0m
/home/zhangs/miniconda3/envs/ptd_esr_env/lib/python3.10/site-packages/airflow/configuration.py:755 UserWarning: Config scheduler.max_tis_per_query (value: 512) should NOT be greater than core.parallelism (value: 32). Will now use core.parallelism as the max task instances per query instead of specified value.
/home/zhangs/miniconda3/envs/ptd_esr_env/lib/python3.10/site-packages/airflow/cli/cli_config.py:971 DeprecationWarning: The namespace option in [kubernetes] has been moved to the namespace option in [kubernetes_executor] - the old setting has been used, but please update your config.
[[34m2023-10-27T14:41:26.371+1300[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /home/zhangs/airflow/dags/ptd_airflow.py[0m
[[34m2023-10-27T14:41:26.723+1300[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-10-27T14:41:26.773+1300[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/zhangs/miniconda3/envs/ptd_esr_env/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2023-10-27T14:41:26.774+1300[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2023-10-27T14:41:27.355+1300[0m] {[34mdagbag.py:[0m346} ERROR[0m - Failed to import: /home/zhangs/miniconda3/envs/ptd_esr_env/lib/python3.10/site-packages/airflow/example_dags/example_subdag_operator.py[0m
Traceback (most recent call last):
  File "/home/zhangs/miniconda3/envs/ptd_esr_env/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/zhangs/miniconda3/envs/ptd_esr_env/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlite3.OperationalError: no such column: slot_pool.include_deferred

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/zhangs/miniconda3/envs/ptd_esr_env/lib/python3.10/site-packages/airflow/models/dagbag.py", line 342, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/zhangs/miniconda3/envs/ptd_esr_env/lib/python3.10/site-packages/airflow/example_dags/example_subdag_operator.py", line 43, in <module>
    section_1 = SubDagOperator(
  File "/home/zhangs/miniconda3/envs/ptd_esr_env/lib/python3.10/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/zhangs/miniconda3/envs/ptd_esr_env/lib/python3.10/site-packages/airflow/utils/session.py", line 77, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/zhangs/miniconda3/envs/ptd_esr_env/lib/python3.10/site-packages/airflow/operators/subdag.py", line 90, in __init__
    self._validate_pool(session)
  File "/home/zhangs/miniconda3/envs/ptd_esr_env/lib/python3.10/site-packages/airflow/operators/subdag.py", line 115, in _validate_pool
    pool = session.query(Pool).filter(Pool.slots == 1).filter(Pool.pool == self.pool).first()
  File "/home/zhangs/miniconda3/envs/ptd_esr_env/lib/python3.10/site-packages/sqlalchemy/orm/query.py", line 2824, in first
    return self.limit(1)._iter().first()
  File "/home/zhangs/miniconda3/envs/ptd_esr_env/lib/python3.10/site-packages/sqlalchemy/orm/query.py", line 2916, in _iter
    result = self.session.execute(
  File "/home/zhangs/miniconda3/envs/ptd_esr_env/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/home/zhangs/miniconda3/envs/ptd_esr_env/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/home/zhangs/miniconda3/envs/ptd_esr_env/lib/python3.10/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/zhangs/miniconda3/envs/ptd_esr_env/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/zhangs/miniconda3/envs/ptd_esr_env/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1953, in _execute_context
    self._handle_dbapi_exception(
  File "/home/zhangs/miniconda3/envs/ptd_esr_env/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2134, in _handle_dbapi_exception
    util.raise_(
  File "/home/zhangs/miniconda3/envs/ptd_esr_env/lib/python3.10/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/zhangs/miniconda3/envs/ptd_esr_env/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/zhangs/miniconda3/envs/ptd_esr_env/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.OperationalError: (sqlite3.OperationalError) no such column: slot_pool.include_deferred
[SQL: SELECT slot_pool.id AS slot_pool_id, slot_pool.pool AS slot_pool_pool, slot_pool.slots AS slot_pool_slots, slot_pool.description AS slot_pool_description, slot_pool.include_deferred AS slot_pool_include_deferred 
FROM slot_pool 
WHERE slot_pool.slots = ? AND slot_pool.pool = ?
 LIMIT ? OFFSET ?]
[parameters: (1, 'default_pool', 1, 0)]
(Background on this error at: https://sqlalche.me/e/14/e3q8)
Traceback (most recent call last):
  File "/home/zhangs/miniconda3/envs/ptd_esr_env/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/zhangs/miniconda3/envs/ptd_esr_env/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlite3.OperationalError: no such column: dag_run.updated_at

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/zhangs/miniconda3/envs/ptd_esr_env/bin/airflow", line 11, in <module>
    sys.exit(main())
  File "/home/zhangs/miniconda3/envs/ptd_esr_env/lib/python3.10/site-packages/airflow/__main__.py", line 59, in main
    args.func(args)
  File "/home/zhangs/miniconda3/envs/ptd_esr_env/lib/python3.10/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/zhangs/miniconda3/envs/ptd_esr_env/lib/python3.10/site-packages/airflow/utils/cli.py", line 113, in wrapper
    return f(*args, **kwargs)
  File "/home/zhangs/miniconda3/envs/ptd_esr_env/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 411, in task_run
    ti, _ = _get_ti(task, args.map_index, exec_date_or_run_id=args.execution_date_or_run_id, pool=args.pool)
  File "/home/zhangs/miniconda3/envs/ptd_esr_env/lib/python3.10/site-packages/airflow/utils/session.py", line 77, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/zhangs/miniconda3/envs/ptd_esr_env/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 175, in _get_ti
    dag_run, dr_created = _get_dag_run(
  File "/home/zhangs/miniconda3/envs/ptd_esr_env/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 111, in _get_dag_run
    dag_run = dag.get_dagrun(run_id=exec_date_or_run_id, session=session)
  File "/home/zhangs/miniconda3/envs/ptd_esr_env/lib/python3.10/site-packages/airflow/utils/session.py", line 74, in wrapper
    return func(*args, **kwargs)
  File "/home/zhangs/miniconda3/envs/ptd_esr_env/lib/python3.10/site-packages/airflow/models/dag.py", line 1491, in get_dagrun
    return session.scalar(query)
  File "/home/zhangs/miniconda3/envs/ptd_esr_env/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
  File "/home/zhangs/miniconda3/envs/ptd_esr_env/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/home/zhangs/miniconda3/envs/ptd_esr_env/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/home/zhangs/miniconda3/envs/ptd_esr_env/lib/python3.10/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/zhangs/miniconda3/envs/ptd_esr_env/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/zhangs/miniconda3/envs/ptd_esr_env/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1953, in _execute_context
    self._handle_dbapi_exception(
  File "/home/zhangs/miniconda3/envs/ptd_esr_env/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2134, in _handle_dbapi_exception
    util.raise_(
  File "/home/zhangs/miniconda3/envs/ptd_esr_env/lib/python3.10/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/zhangs/miniconda3/envs/ptd_esr_env/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/zhangs/miniconda3/envs/ptd_esr_env/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.OperationalError: (sqlite3.OperationalError) no such column: dag_run.updated_at
[SQL: SELECT dag_run.state, dag_run.id, dag_run.dag_id, dag_run.queued_at, dag_run.execution_date, dag_run.start_date, dag_run.end_date, dag_run.run_id, dag_run.creating_job_id, dag_run.external_trigger, dag_run.run_type, dag_run.conf, dag_run.data_interval_start, dag_run.data_interval_end, dag_run.last_scheduling_decision, dag_run.dag_hash, dag_run.log_template_id, dag_run.updated_at 
FROM dag_run 
WHERE dag_run.dag_id = ? AND dag_run.run_id = ?]
[parameters: ('hello_world', 'manual__2023-10-27T01:40:07.188370+00:00')]
(Background on this error at: https://sqlalche.me/e/14/e3q8)
[[34m2023-10-27 14:41:28,476[0m] {[34msequential_executor.py:[0m68} ERROR[0m - Failed to execute task Command '['airflow', 'tasks', 'run', 'hello_world', 'hello_world_task', 'manual__2023-10-27T01:40:07.188370+00:00', '--local', '--subdir', 'DAGS_FOLDER/ptd_airflow.py']' returned non-zero exit status 1..[0m
[[34m2023-10-27 14:41:28,478[0m] {[34mscheduler_job.py:[0m588} INFO[0m - Executor reports execution of hello_world.hello_world_task run_id=manual__2023-10-27T01:40:07.188370+00:00 exited with status failed for try_number 1[0m
[[34m2023-10-27 14:41:28,509[0m] {[34mscheduler_job.py:[0m631} INFO[0m - TaskInstance Finished: dag_id=hello_world, task_id=hello_world_task, run_id=manual__2023-10-27T01:40:07.188370+00:00, map_index=-1, run_start_date=None, run_end_date=None, run_duration=None, state=queued, executor_state=failed, try_number=1, max_tries=0, job_id=None, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-10-27 01:41:24.090501+00:00, queued_by_job_id=18, pid=None[0m
[[34m2023-10-27 14:41:28,510[0m] {[34mscheduler_job.py:[0m673} ERROR[0m - Executor reports task instance <TaskInstance: hello_world.hello_world_task manual__2023-10-27T01:40:07.188370+00:00 [queued]> finished (failed) although the task says its queued. (Info: None) Was the task killed externally?[0m
[[34m2023-10-27 14:41:28,511[0m] {[34mtaskinstance.py:[0m1853} ERROR[0m - Executor reports task instance <TaskInstance: hello_world.hello_world_task manual__2023-10-27T01:40:07.188370+00:00 [queued]> finished (failed) although the task says its queued. (Info: None) Was the task killed externally?[0m
[[34m2023-10-27 14:41:28,753[0m] {[34mtaskinstance.py:[0m1401} INFO[0m - Marking task as FAILED. dag_id=hello_world, task_id=hello_world_task, execution_date=20231027T014007, start_date=, end_date=20231027T014128[0m
[[34m2023-10-27 14:41:29,431[0m] {[34mdagrun.py:[0m578} ERROR[0m - Marking run <DagRun hello_world @ 2023-10-27 01:40:07.188370+00:00: manual__2023-10-27T01:40:07.188370+00:00, state:running, queued_at: 2023-10-27 01:40:07.320977+00:00. externally triggered: True> failed[0m
[[34m2023-10-27 14:41:29,432[0m] {[34mdagrun.py:[0m644} INFO[0m - DagRun Finished: dag_id=hello_world, execution_date=2023-10-27 01:40:07.188370+00:00, run_id=manual__2023-10-27T01:40:07.188370+00:00, run_start_date=2023-10-27 01:41:23.954259+00:00, run_end_date=2023-10-27 01:41:29.432139+00:00, run_duration=5.47788, state=failed, external_trigger=True, run_type=manual, data_interval_start=2023-10-27 01:40:07.188370+00:00, data_interval_end=2023-10-27 01:40:07.188370+00:00, dag_hash=8f854b03cac532842b5b2006c82363e1[0m
[[34m2023-10-27 14:41:29,442[0m] {[34mdag.py:[0m3336} INFO[0m - Setting next_dagrun for hello_world to None, run_after=None[0m
[[34m2023-10-27 14:46:23,439[0m] {[34mscheduler_job.py:[0m1380} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2023-10-27 14:51:24,056[0m] {[34mscheduler_job.py:[0m1380} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2023-10-27 14:56:24,527[0m] {[34mscheduler_job.py:[0m1380} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2023-10-27 15:01:24,937[0m] {[34mscheduler_job.py:[0m1380} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2023-10-27 15:06:25,449[0m] {[34mscheduler_job.py:[0m1380} INFO[0m - Resetting orphaned tasks for active dag runs[0m
Process ForkProcess-2:
Traceback (most recent call last):
  File "/home/zhangs/miniconda3/envs/airflow/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/zhangs/miniconda3/envs/airflow/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlite3.OperationalError: no such table: dag_warning

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/zhangs/miniconda3/envs/airflow/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/zhangs/miniconda3/envs/airflow/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/zhangs/miniconda3/envs/airflow/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 257, in _run_processor_manager
    processor_manager.start()
  File "/home/zhangs/miniconda3/envs/airflow/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/zhangs/miniconda3/envs/airflow/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 603, in _run_parsing_loop
    DagWarning.purge_inactive_dag_warnings()
  File "/home/zhangs/miniconda3/envs/airflow/lib/python3.9/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/zhangs/miniconda3/envs/airflow/lib/python3.9/site-packages/airflow/models/dagwarning.py", line 82, in purge_inactive_dag_warnings
    query.delete(synchronize_session=False)
  File "/home/zhangs/miniconda3/envs/airflow/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 3222, in delete
    result = self.session.execute(
  File "/home/zhangs/miniconda3/envs/airflow/lib/python3.9/site-packages/sqlalchemy/orm/session.py", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/home/zhangs/miniconda3/envs/airflow/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/home/zhangs/miniconda3/envs/airflow/lib/python3.9/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/zhangs/miniconda3/envs/airflow/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/zhangs/miniconda3/envs/airflow/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1953, in _execute_context
    self._handle_dbapi_exception(
  File "/home/zhangs/miniconda3/envs/airflow/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 2134, in _handle_dbapi_exception
    util.raise_(
  File "/home/zhangs/miniconda3/envs/airflow/lib/python3.9/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/zhangs/miniconda3/envs/airflow/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/zhangs/miniconda3/envs/airflow/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.OperationalError: (sqlite3.OperationalError) no such table: dag_warning
[SQL: DELETE FROM dag_warning WHERE dag_warning.dag_id IN (SELECT dag.dag_id 
FROM dag 
WHERE dag.is_active = 0)]
(Background on this error at: https://sqlalche.me/e/14/e3q8)
[[34m2023-10-27 15:07:41,610[0m] {[34mmanager.py:[0m288} WARNING[0m - DagFileProcessorManager (PID=3124608) exited with exit code 1 - re-launching[0m
[[34m2023-10-27 15:07:41,618[0m] {[34mmanager.py:[0m163} INFO[0m - Launched DagFileProcessorManager with pid: 4122851[0m
[[34m2023-10-27 15:07:41,633[0m] {[34msettings.py:[0m58} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2023-10-27T15:07:41.657+1300] {manager.py:409} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2) when using sqlite. So we set parallelism to 1.
/home/zhangs/miniconda3/envs/airflow/lib/python3.9/site-packages/airflow/models/base.py:49 MovedIn20Warning: Deprecated API features detected! These feature(s) are not compatible with SQLAlchemy 2.0. To prevent incompatible upgrades prior to updating applications, ensure requirements files are pinned to "sqlalchemy<2.0". Set environment variable SQLALCHEMY_WARN_20=1 to show all deprecation warnings.  Set environment variable SQLALCHEMY_SILENCE_UBER_WARNING=1 to silence this message. (Background on SQLAlchemy 2.0 at: https://sqlalche.me/e/b8d9)
ERROR: You need to upgrade the database. Please run `airflow db upgrade`. Make sure the command is run using Airflow version 2.4.3.
/home/zhangs/miniconda3/envs/airflow/lib/python3.9/site-packages/airflow/models/base.py:49 MovedIn20Warning: Deprecated API features detected! These feature(s) are not compatible with SQLAlchemy 2.0. To prevent incompatible upgrades prior to updating applications, ensure requirements files are pinned to "sqlalchemy<2.0". Set environment variable SQLALCHEMY_WARN_20=1 to show all deprecation warnings.  Set environment variable SQLALCHEMY_SILENCE_UBER_WARNING=1 to silence this message. (Background on SQLAlchemy 2.0 at: https://sqlalche.me/e/b8d9)
  ____________       _____________
 ____    |__( )_________  __/__  /________      __
____  /| |_  /__  ___/_  /_ __  /_  __ \_ | /| / /
___  ___ |  / _  /   _  __/ _  / / /_/ /_ |/ |/ /
 _/_/  |_/_/  /_/    /_/    /_/  \____/____/|__/
[2023-10-27 16:24:49 +1300] [2927247] [INFO] Starting gunicorn 21.2.0
[2023-10-27 16:24:49 +1300] [2927247] [INFO] Listening at: http://[::]:8083 (2927247)
[2023-10-27 16:24:49 +1300] [2927247] [INFO] Using worker: sync
[[34m2023-10-27 16:24:49,417[0m] {[34mscheduler_job.py:[0m700} INFO[0m - Starting the scheduler[0m
[[34m2023-10-27 16:24:49,417[0m] {[34mscheduler_job.py:[0m705} INFO[0m - Processing each file at most -1 times[0m
[[34m2023-10-27 16:24:49,422[0m] {[34mexecutor_loader.py:[0m107} INFO[0m - Loaded executor: SequentialExecutor[0m
[2023-10-27 16:24:49 +1300] [2927279] [INFO] Booting worker with pid: 2927279
[[34m2023-10-27 16:24:49,428[0m] {[34mmanager.py:[0m163} INFO[0m - Launched DagFileProcessorManager with pid: 2927287[0m
[[34m2023-10-27 16:24:49,430[0m] {[34mscheduler_job.py:[0m1380} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2023-10-27 16:24:49,434[0m] {[34msettings.py:[0m58} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2023-10-27T16:24:49.449+1300] {manager.py:409} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2) when using sqlite. So we set parallelism to 1.
[2023-10-27 16:24:49 +1300] [2927311] [INFO] Booting worker with pid: 2927311
[[34m2023-10-27 16:25:19,222[0m] {[34mscheduler_job.py:[0m346} INFO[0m - 1 tasks up for execution:
	<TaskInstance: hello_world.hello_world_task manual__2023-10-27T03:25:17.308193+00:00 [scheduled]>[0m
[[34m2023-10-27 16:25:19,223[0m] {[34mscheduler_job.py:[0m411} INFO[0m - DAG hello_world has 0/16 running and queued tasks[0m
[[34m2023-10-27 16:25:19,223[0m] {[34mscheduler_job.py:[0m497} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: hello_world.hello_world_task manual__2023-10-27T03:25:17.308193+00:00 [scheduled]>[0m
[[34m2023-10-27 16:25:19,232[0m] {[34mscheduler_job.py:[0m536} INFO[0m - Sending TaskInstanceKey(dag_id='hello_world', task_id='hello_world_task', run_id='manual__2023-10-27T03:25:17.308193+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-10-27 16:25:19,232[0m] {[34mbase_executor.py:[0m95} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'hello_world', 'hello_world_task', 'manual__2023-10-27T03:25:17.308193+00:00', '--local', '--subdir', 'DAGS_FOLDER/ptd_airflow.py'][0m
[[34m2023-10-27 16:25:19,243[0m] {[34msequential_executor.py:[0m61} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'hello_world', 'hello_world_task', 'manual__2023-10-27T03:25:17.308193+00:00', '--local', '--subdir', 'DAGS_FOLDER/ptd_airflow.py'][0m
/home/zhangs/miniconda3/envs/ptd_esr_env/lib/python3.10/site-packages/airflow/models/base.py:49 MovedIn20Warning: Deprecated API features detected! These feature(s) are not compatible with SQLAlchemy 2.0. To prevent incompatible upgrades prior to updating applications, ensure requirements files are pinned to "sqlalchemy<2.0". Set environment variable SQLALCHEMY_WARN_20=1 to show all deprecation warnings.  Set environment variable SQLALCHEMY_SILENCE_UBER_WARNING=1 to silence this message. (Background on SQLAlchemy 2.0 at: https://sqlalche.me/e/b8d9)
[[34m2023-10-27 16:25:21,011[0m] {[34mdagbag.py:[0m537} INFO[0m - Filling up the DagBag from /home/zhangs/airflow/dags/ptd_airflow.py[0m
[[34m2023-10-27 16:25:21,625[0m] {[34mtask_command.py:[0m376} INFO[0m - Running <TaskInstance: hello_world.hello_world_task manual__2023-10-27T03:25:17.308193+00:00 [queued]> on host kscprod-data1.esr.cri.nz[0m
[[34m2023-10-27 16:25:22,489[0m] {[34mscheduler_job.py:[0m588} INFO[0m - Executor reports execution of hello_world.hello_world_task run_id=manual__2023-10-27T03:25:17.308193+00:00 exited with status success for try_number 1[0m
[[34m2023-10-27 16:25:22,736[0m] {[34mscheduler_job.py:[0m631} INFO[0m - TaskInstance Finished: dag_id=hello_world, task_id=hello_world_task, run_id=manual__2023-10-27T03:25:17.308193+00:00, map_index=-1, run_start_date=2023-10-27 03:25:21.752808+00:00, run_end_date=2023-10-27 03:25:22.197159+00:00, run_duration=0.444351, state=success, executor_state=success, try_number=1, max_tries=0, job_id=2, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-10-27 03:25:19.225050+00:00, queued_by_job_id=1, pid=2945789[0m
[[34m2023-10-27 16:25:23,281[0m] {[34mdagrun.py:[0m597} INFO[0m - Marking run <DagRun hello_world @ 2023-10-27 03:25:17.308193+00:00: manual__2023-10-27T03:25:17.308193+00:00, state:running, queued_at: 2023-10-27 03:25:17.407203+00:00. externally triggered: True> successful[0m
[[34m2023-10-27 16:25:23,282[0m] {[34mdagrun.py:[0m644} INFO[0m - DagRun Finished: dag_id=hello_world, execution_date=2023-10-27 03:25:17.308193+00:00, run_id=manual__2023-10-27T03:25:17.308193+00:00, run_start_date=2023-10-27 03:25:19.101567+00:00, run_end_date=2023-10-27 03:25:23.282618+00:00, run_duration=4.181051, state=success, external_trigger=True, run_type=manual, data_interval_start=2023-10-27 03:25:17.308193+00:00, data_interval_end=2023-10-27 03:25:17.308193+00:00, dag_hash=8f854b03cac532842b5b2006c82363e1[0m
[[34m2023-10-27 16:25:23,293[0m] {[34mdag.py:[0m3336} INFO[0m - Setting next_dagrun for hello_world to None, run_after=None[0m
/home/zhangs/miniconda3/envs/airflow/lib/python3.9/site-packages/airflow/models/base.py:49 MovedIn20Warning: Deprecated API features detected! These feature(s) are not compatible with SQLAlchemy 2.0. To prevent incompatible upgrades prior to updating applications, ensure requirements files are pinned to "sqlalchemy<2.0". Set environment variable SQLALCHEMY_WARN_20=1 to show all deprecation warnings.  Set environment variable SQLALCHEMY_SILENCE_UBER_WARNING=1 to silence this message. (Background on SQLAlchemy 2.0 at: https://sqlalche.me/e/b8d9)
  ____________       _____________
 ____    |__( )_________  __/__  /________      __
____  /| |_  /__  ___/_  /_ __  /_  __ \_ | /| / /
___  ___ |  / _  /   _  __/ _  / / /_/ /_ |/ |/ /
 _/_/  |_/_/  /_/    /_/    /_/  \____/____/|__/
[2023-10-27 16:29:02 +1300] [3084654] [INFO] Starting gunicorn 21.2.0
[2023-10-27 16:29:02 +1300] [3084654] [INFO] Listening at: http://[::]:8083 (3084654)
[2023-10-27 16:29:02 +1300] [3084654] [INFO] Using worker: sync
[[34m2023-10-27 16:29:02,839[0m] {[34mscheduler_job.py:[0m700} INFO[0m - Starting the scheduler[0m
[[34m2023-10-27 16:29:02,839[0m] {[34mscheduler_job.py:[0m705} INFO[0m - Processing each file at most -1 times[0m
[2023-10-27 16:29:02 +1300] [3084683] [INFO] Booting worker with pid: 3084683
[[34m2023-10-27 16:29:02,843[0m] {[34mexecutor_loader.py:[0m107} INFO[0m - Loaded executor: SequentialExecutor[0m
[[34m2023-10-27 16:29:02,850[0m] {[34mmanager.py:[0m163} INFO[0m - Launched DagFileProcessorManager with pid: 3084692[0m
[[34m2023-10-27 16:29:02,851[0m] {[34mscheduler_job.py:[0m1380} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[2023-10-27 16:29:02 +1300] [3084689] [INFO] Booting worker with pid: 3084689
[[34m2023-10-27 16:29:02,860[0m] {[34msettings.py:[0m58} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2023-10-27T16:29:02.876+1300] {manager.py:409} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2) when using sqlite. So we set parallelism to 1.
[[34m2023-10-27 16:30:49,377[0m] {[34mdag.py:[0m3336} INFO[0m - Setting next_dagrun for call_ptd to 2023-10-26T06:00:00+00:00, run_after=2023-10-26T12:00:00+00:00[0m
[[34m2023-10-27 16:30:49,572[0m] {[34mscheduler_job.py:[0m346} INFO[0m - 2 tasks up for execution:
	<TaskInstance: call_ptd.ptd_job scheduled__2023-10-26T00:00:00+00:00 [scheduled]>
	<TaskInstance: call_ptd.ptd_job manual__2023-10-27T03:30:48.140808+00:00 [scheduled]>[0m
[[34m2023-10-27 16:30:49,572[0m] {[34mscheduler_job.py:[0m411} INFO[0m - DAG call_ptd has 0/16 running and queued tasks[0m
[[34m2023-10-27 16:30:49,572[0m] {[34mscheduler_job.py:[0m411} INFO[0m - DAG call_ptd has 1/16 running and queued tasks[0m
[[34m2023-10-27 16:30:49,573[0m] {[34mscheduler_job.py:[0m497} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: call_ptd.ptd_job scheduled__2023-10-26T00:00:00+00:00 [scheduled]>
	<TaskInstance: call_ptd.ptd_job manual__2023-10-27T03:30:48.140808+00:00 [scheduled]>[0m
[[34m2023-10-27 16:30:49,582[0m] {[34mscheduler_job.py:[0m536} INFO[0m - Sending TaskInstanceKey(dag_id='call_ptd', task_id='ptd_job', run_id='scheduled__2023-10-26T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-10-27 16:30:49,582[0m] {[34mbase_executor.py:[0m95} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'call_ptd', 'ptd_job', 'scheduled__2023-10-26T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/ptd_airflow.py'][0m
[[34m2023-10-27 16:30:49,583[0m] {[34mscheduler_job.py:[0m536} INFO[0m - Sending TaskInstanceKey(dag_id='call_ptd', task_id='ptd_job', run_id='manual__2023-10-27T03:30:48.140808+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-10-27 16:30:49,584[0m] {[34mbase_executor.py:[0m95} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'call_ptd', 'ptd_job', 'manual__2023-10-27T03:30:48.140808+00:00', '--local', '--subdir', 'DAGS_FOLDER/ptd_airflow.py'][0m
[[34m2023-10-27 16:30:49,594[0m] {[34msequential_executor.py:[0m61} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'call_ptd', 'ptd_job', 'scheduled__2023-10-26T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/ptd_airflow.py'][0m
/home/zhangs/miniconda3/envs/ptd_esr_env/lib/python3.10/site-packages/airflow/models/base.py:49 MovedIn20Warning: Deprecated API features detected! These feature(s) are not compatible with SQLAlchemy 2.0. To prevent incompatible upgrades prior to updating applications, ensure requirements files are pinned to "sqlalchemy<2.0". Set environment variable SQLALCHEMY_WARN_20=1 to show all deprecation warnings.  Set environment variable SQLALCHEMY_SILENCE_UBER_WARNING=1 to silence this message. (Background on SQLAlchemy 2.0 at: https://sqlalche.me/e/b8d9)
[[34m2023-10-27 16:30:51,071[0m] {[34mdagbag.py:[0m537} INFO[0m - Filling up the DagBag from /home/zhangs/airflow/dags/ptd_airflow.py[0m
[[34m2023-10-27 16:30:51,600[0m] {[34mtask_command.py:[0m376} INFO[0m - Running <TaskInstance: call_ptd.ptd_job scheduled__2023-10-26T00:00:00+00:00 [queued]> on host kscprod-data1.esr.cri.nz[0m
[[34m2023-10-27 16:30:52,422[0m] {[34msequential_executor.py:[0m61} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'call_ptd', 'ptd_job', 'manual__2023-10-27T03:30:48.140808+00:00', '--local', '--subdir', 'DAGS_FOLDER/ptd_airflow.py'][0m
/home/zhangs/miniconda3/envs/ptd_esr_env/lib/python3.10/site-packages/airflow/models/base.py:49 MovedIn20Warning: Deprecated API features detected! These feature(s) are not compatible with SQLAlchemy 2.0. To prevent incompatible upgrades prior to updating applications, ensure requirements files are pinned to "sqlalchemy<2.0". Set environment variable SQLALCHEMY_WARN_20=1 to show all deprecation warnings.  Set environment variable SQLALCHEMY_SILENCE_UBER_WARNING=1 to silence this message. (Background on SQLAlchemy 2.0 at: https://sqlalche.me/e/b8d9)
[[34m2023-10-27 16:30:53,472[0m] {[34mdagbag.py:[0m537} INFO[0m - Filling up the DagBag from /home/zhangs/airflow/dags/ptd_airflow.py[0m
[[34m2023-10-27 16:30:53,907[0m] {[34mtask_command.py:[0m376} INFO[0m - Running <TaskInstance: call_ptd.ptd_job manual__2023-10-27T03:30:48.140808+00:00 [queued]> on host kscprod-data1.esr.cri.nz[0m
[[34m2023-10-27 16:30:54,709[0m] {[34mscheduler_job.py:[0m588} INFO[0m - Executor reports execution of call_ptd.ptd_job run_id=scheduled__2023-10-26T00:00:00+00:00 exited with status success for try_number 1[0m
[[34m2023-10-27 16:30:54,710[0m] {[34mscheduler_job.py:[0m588} INFO[0m - Executor reports execution of call_ptd.ptd_job run_id=manual__2023-10-27T03:30:48.140808+00:00 exited with status success for try_number 1[0m
[[34m2023-10-27 16:30:54,744[0m] {[34mscheduler_job.py:[0m631} INFO[0m - TaskInstance Finished: dag_id=call_ptd, task_id=ptd_job, run_id=manual__2023-10-27T03:30:48.140808+00:00, map_index=-1, run_start_date=2023-10-27 03:30:54.020673+00:00, run_end_date=2023-10-27 03:30:54.385420+00:00, run_duration=0.364747, state=failed, executor_state=success, try_number=1, max_tries=0, job_id=3, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-10-27 03:30:49.574533+00:00, queued_by_job_id=1, pid=3151829[0m
[[34m2023-10-27 16:30:54,744[0m] {[34mscheduler_job.py:[0m631} INFO[0m - TaskInstance Finished: dag_id=call_ptd, task_id=ptd_job, run_id=scheduled__2023-10-26T00:00:00+00:00, map_index=-1, run_start_date=2023-10-27 03:30:51.722675+00:00, run_end_date=2023-10-27 03:30:52.085907+00:00, run_duration=0.363232, state=failed, executor_state=success, try_number=1, max_tries=0, job_id=2, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-10-27 03:30:49.574533+00:00, queued_by_job_id=1, pid=3150471[0m
[[34m2023-10-27 16:30:55,215[0m] {[34mdag.py:[0m3336} INFO[0m - Setting next_dagrun for call_ptd to 2023-10-26T12:00:00+00:00, run_after=2023-10-26T18:00:00+00:00[0m
[[34m2023-10-27 16:30:55,317[0m] {[34mdagrun.py:[0m578} ERROR[0m - Marking run <DagRun call_ptd @ 2023-10-26 00:00:00+00:00: scheduled__2023-10-26T00:00:00+00:00, state:running, queued_at: 2023-10-27 03:30:49.352334+00:00. externally triggered: False> failed[0m
[[34m2023-10-27 16:30:55,318[0m] {[34mdagrun.py:[0m644} INFO[0m - DagRun Finished: dag_id=call_ptd, execution_date=2023-10-26 00:00:00+00:00, run_id=scheduled__2023-10-26T00:00:00+00:00, run_start_date=2023-10-27 03:30:49.416074+00:00, run_end_date=2023-10-27 03:30:55.318780+00:00, run_duration=5.902706, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2023-10-26 00:00:00+00:00, data_interval_end=2023-10-26 06:00:00+00:00, dag_hash=73809f00374e264528c5ff396bb50051[0m
[[34m2023-10-27 16:30:55,323[0m] {[34mdag.py:[0m3336} INFO[0m - Setting next_dagrun for call_ptd to 2023-10-26T06:00:00+00:00, run_after=2023-10-26T12:00:00+00:00[0m
[[34m2023-10-27 16:30:55,326[0m] {[34mdagrun.py:[0m578} ERROR[0m - Marking run <DagRun call_ptd @ 2023-10-27 03:30:48.140808+00:00: manual__2023-10-27T03:30:48.140808+00:00, state:running, queued_at: 2023-10-27 03:30:48.261883+00:00. externally triggered: True> failed[0m
[[34m2023-10-27 16:30:55,326[0m] {[34mdagrun.py:[0m644} INFO[0m - DagRun Finished: dag_id=call_ptd, execution_date=2023-10-27 03:30:48.140808+00:00, run_id=manual__2023-10-27T03:30:48.140808+00:00, run_start_date=2023-10-27 03:30:49.416473+00:00, run_end_date=2023-10-27 03:30:55.326524+00:00, run_duration=5.910051, state=failed, external_trigger=True, run_type=manual, data_interval_start=2023-10-26 21:30:48.140808+00:00, data_interval_end=2023-10-27 03:30:48.140808+00:00, dag_hash=73809f00374e264528c5ff396bb50051[0m
[[34m2023-10-27 16:30:55,329[0m] {[34mdag.py:[0m3336} INFO[0m - Setting next_dagrun for call_ptd to 2023-10-27T03:30:48.140808+00:00, run_after=2023-10-27T09:30:48.140808+00:00[0m
[[34m2023-10-27 16:30:55,378[0m] {[34mscheduler_job.py:[0m346} INFO[0m - 1 tasks up for execution:
	<TaskInstance: call_ptd.ptd_job scheduled__2023-10-26T06:00:00+00:00 [scheduled]>[0m
[[34m2023-10-27 16:30:55,378[0m] {[34mscheduler_job.py:[0m411} INFO[0m - DAG call_ptd has 0/16 running and queued tasks[0m
[[34m2023-10-27 16:30:55,379[0m] {[34mscheduler_job.py:[0m497} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: call_ptd.ptd_job scheduled__2023-10-26T06:00:00+00:00 [scheduled]>[0m
[[34m2023-10-27 16:30:55,389[0m] {[34mscheduler_job.py:[0m536} INFO[0m - Sending TaskInstanceKey(dag_id='call_ptd', task_id='ptd_job', run_id='scheduled__2023-10-26T06:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-10-27 16:30:55,389[0m] {[34mbase_executor.py:[0m95} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'call_ptd', 'ptd_job', 'scheduled__2023-10-26T06:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/ptd_airflow.py'][0m
[[34m2023-10-27 16:30:55,400[0m] {[34msequential_executor.py:[0m61} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'call_ptd', 'ptd_job', 'scheduled__2023-10-26T06:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/ptd_airflow.py'][0m
/home/zhangs/miniconda3/envs/ptd_esr_env/lib/python3.10/site-packages/airflow/models/base.py:49 MovedIn20Warning: Deprecated API features detected! These feature(s) are not compatible with SQLAlchemy 2.0. To prevent incompatible upgrades prior to updating applications, ensure requirements files are pinned to "sqlalchemy<2.0". Set environment variable SQLALCHEMY_WARN_20=1 to show all deprecation warnings.  Set environment variable SQLALCHEMY_SILENCE_UBER_WARNING=1 to silence this message. (Background on SQLAlchemy 2.0 at: https://sqlalche.me/e/b8d9)
[[34m2023-10-27 16:30:56,380[0m] {[34mdagbag.py:[0m537} INFO[0m - Filling up the DagBag from /home/zhangs/airflow/dags/ptd_airflow.py[0m
[[34m2023-10-27 16:30:56,805[0m] {[34mtask_command.py:[0m376} INFO[0m - Running <TaskInstance: call_ptd.ptd_job scheduled__2023-10-26T06:00:00+00:00 [queued]> on host kscprod-data1.esr.cri.nz[0m
[[34m2023-10-27 16:30:57,539[0m] {[34mscheduler_job.py:[0m588} INFO[0m - Executor reports execution of call_ptd.ptd_job run_id=scheduled__2023-10-26T06:00:00+00:00 exited with status success for try_number 1[0m
[[34m2023-10-27 16:30:57,573[0m] {[34mscheduler_job.py:[0m631} INFO[0m - TaskInstance Finished: dag_id=call_ptd, task_id=ptd_job, run_id=scheduled__2023-10-26T06:00:00+00:00, map_index=-1, run_start_date=2023-10-27 03:30:56.902849+00:00, run_end_date=2023-10-27 03:30:57.249794+00:00, run_duration=0.346945, state=failed, executor_state=success, try_number=1, max_tries=0, job_id=4, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-10-27 03:30:55.380722+00:00, queued_by_job_id=1, pid=3153336[0m
[[34m2023-10-27 16:30:57,981[0m] {[34mdag.py:[0m3336} INFO[0m - Setting next_dagrun for call_ptd to 2023-10-26T18:00:00+00:00, run_after=2023-10-27T00:00:00+00:00[0m
[[34m2023-10-27 16:30:58,130[0m] {[34mdagrun.py:[0m578} ERROR[0m - Marking run <DagRun call_ptd @ 2023-10-26 06:00:00+00:00: scheduled__2023-10-26T06:00:00+00:00, state:running, queued_at: 2023-10-27 03:30:55.204030+00:00. externally triggered: False> failed[0m
[[34m2023-10-27 16:30:58,130[0m] {[34mdagrun.py:[0m644} INFO[0m - DagRun Finished: dag_id=call_ptd, execution_date=2023-10-26 06:00:00+00:00, run_id=scheduled__2023-10-26T06:00:00+00:00, run_start_date=2023-10-27 03:30:55.254478+00:00, run_end_date=2023-10-27 03:30:58.130593+00:00, run_duration=2.876115, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2023-10-26 06:00:00+00:00, data_interval_end=2023-10-26 12:00:00+00:00, dag_hash=73809f00374e264528c5ff396bb50051[0m
[[34m2023-10-27 16:30:58,139[0m] {[34mdag.py:[0m3336} INFO[0m - Setting next_dagrun for call_ptd to 2023-10-26T12:00:00+00:00, run_after=2023-10-26T18:00:00+00:00[0m
[[34m2023-10-27 16:30:58,212[0m] {[34mscheduler_job.py:[0m346} INFO[0m - 1 tasks up for execution:
	<TaskInstance: call_ptd.ptd_job scheduled__2023-10-26T12:00:00+00:00 [scheduled]>[0m
[[34m2023-10-27 16:30:58,213[0m] {[34mscheduler_job.py:[0m411} INFO[0m - DAG call_ptd has 0/16 running and queued tasks[0m
[[34m2023-10-27 16:30:58,213[0m] {[34mscheduler_job.py:[0m497} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: call_ptd.ptd_job scheduled__2023-10-26T12:00:00+00:00 [scheduled]>[0m
[[34m2023-10-27 16:30:58,221[0m] {[34mscheduler_job.py:[0m536} INFO[0m - Sending TaskInstanceKey(dag_id='call_ptd', task_id='ptd_job', run_id='scheduled__2023-10-26T12:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-10-27 16:30:58,221[0m] {[34mbase_executor.py:[0m95} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'call_ptd', 'ptd_job', 'scheduled__2023-10-26T12:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/ptd_airflow.py'][0m
[[34m2023-10-27 16:30:58,249[0m] {[34msequential_executor.py:[0m61} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'call_ptd', 'ptd_job', 'scheduled__2023-10-26T12:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/ptd_airflow.py'][0m
/home/zhangs/miniconda3/envs/ptd_esr_env/lib/python3.10/site-packages/airflow/models/base.py:49 MovedIn20Warning: Deprecated API features detected! These feature(s) are not compatible with SQLAlchemy 2.0. To prevent incompatible upgrades prior to updating applications, ensure requirements files are pinned to "sqlalchemy<2.0". Set environment variable SQLALCHEMY_WARN_20=1 to show all deprecation warnings.  Set environment variable SQLALCHEMY_SILENCE_UBER_WARNING=1 to silence this message. (Background on SQLAlchemy 2.0 at: https://sqlalche.me/e/b8d9)
[[34m2023-10-27 16:30:59,456[0m] {[34mdagbag.py:[0m537} INFO[0m - Filling up the DagBag from /home/zhangs/airflow/dags/ptd_airflow.py[0m
[[34m2023-10-27 16:30:59,856[0m] {[34mtask_command.py:[0m376} INFO[0m - Running <TaskInstance: call_ptd.ptd_job scheduled__2023-10-26T12:00:00+00:00 [queued]> on host kscprod-data1.esr.cri.nz[0m
[[34m2023-10-27 16:31:00,638[0m] {[34mscheduler_job.py:[0m588} INFO[0m - Executor reports execution of call_ptd.ptd_job run_id=scheduled__2023-10-26T12:00:00+00:00 exited with status success for try_number 1[0m
[[34m2023-10-27 16:31:00,681[0m] {[34mscheduler_job.py:[0m631} INFO[0m - TaskInstance Finished: dag_id=call_ptd, task_id=ptd_job, run_id=scheduled__2023-10-26T12:00:00+00:00, map_index=-1, run_start_date=2023-10-27 03:30:59.956120+00:00, run_end_date=2023-10-27 03:31:00.314205+00:00, run_duration=0.358085, state=failed, executor_state=success, try_number=1, max_tries=0, job_id=5, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-10-27 03:30:58.214655+00:00, queued_by_job_id=1, pid=3155038[0m
[[34m2023-10-27 16:31:01,286[0m] {[34mdag.py:[0m3336} INFO[0m - Setting next_dagrun for call_ptd to 2023-10-27T00:00:00+00:00, run_after=2023-10-27T06:00:00+00:00[0m
[[34m2023-10-27 16:31:01,421[0m] {[34mdagrun.py:[0m578} ERROR[0m - Marking run <DagRun call_ptd @ 2023-10-26 12:00:00+00:00: scheduled__2023-10-26T12:00:00+00:00, state:running, queued_at: 2023-10-27 03:30:57.967300+00:00. externally triggered: False> failed[0m
[[34m2023-10-27 16:31:01,421[0m] {[34mdagrun.py:[0m644} INFO[0m - DagRun Finished: dag_id=call_ptd, execution_date=2023-10-26 12:00:00+00:00, run_id=scheduled__2023-10-26T12:00:00+00:00, run_start_date=2023-10-27 03:30:58.038541+00:00, run_end_date=2023-10-27 03:31:01.421500+00:00, run_duration=3.382959, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2023-10-26 12:00:00+00:00, data_interval_end=2023-10-26 18:00:00+00:00, dag_hash=73809f00374e264528c5ff396bb50051[0m
[[34m2023-10-27 16:31:01,424[0m] {[34mdag.py:[0m3336} INFO[0m - Setting next_dagrun for call_ptd to 2023-10-26T18:00:00+00:00, run_after=2023-10-27T00:00:00+00:00[0m
[[34m2023-10-27 16:31:01,483[0m] {[34mscheduler_job.py:[0m346} INFO[0m - 1 tasks up for execution:
	<TaskInstance: call_ptd.ptd_job scheduled__2023-10-26T18:00:00+00:00 [scheduled]>[0m
[[34m2023-10-27 16:31:01,483[0m] {[34mscheduler_job.py:[0m411} INFO[0m - DAG call_ptd has 0/16 running and queued tasks[0m
[[34m2023-10-27 16:31:01,483[0m] {[34mscheduler_job.py:[0m497} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: call_ptd.ptd_job scheduled__2023-10-26T18:00:00+00:00 [scheduled]>[0m
[[34m2023-10-27 16:31:01,492[0m] {[34mscheduler_job.py:[0m536} INFO[0m - Sending TaskInstanceKey(dag_id='call_ptd', task_id='ptd_job', run_id='scheduled__2023-10-26T18:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-10-27 16:31:01,492[0m] {[34mbase_executor.py:[0m95} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'call_ptd', 'ptd_job', 'scheduled__2023-10-26T18:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/ptd_airflow.py'][0m
[[34m2023-10-27 16:31:01,502[0m] {[34msequential_executor.py:[0m61} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'call_ptd', 'ptd_job', 'scheduled__2023-10-26T18:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/ptd_airflow.py'][0m
/home/zhangs/miniconda3/envs/ptd_esr_env/lib/python3.10/site-packages/airflow/models/base.py:49 MovedIn20Warning: Deprecated API features detected! These feature(s) are not compatible with SQLAlchemy 2.0. To prevent incompatible upgrades prior to updating applications, ensure requirements files are pinned to "sqlalchemy<2.0". Set environment variable SQLALCHEMY_WARN_20=1 to show all deprecation warnings.  Set environment variable SQLALCHEMY_SILENCE_UBER_WARNING=1 to silence this message. (Background on SQLAlchemy 2.0 at: https://sqlalche.me/e/b8d9)
[[34m2023-10-27 16:31:02,511[0m] {[34mdagbag.py:[0m537} INFO[0m - Filling up the DagBag from /home/zhangs/airflow/dags/ptd_airflow.py[0m
[[34m2023-10-27 16:31:02,930[0m] {[34mtask_command.py:[0m376} INFO[0m - Running <TaskInstance: call_ptd.ptd_job scheduled__2023-10-26T18:00:00+00:00 [queued]> on host kscprod-data1.esr.cri.nz[0m
[[34m2023-10-27 16:31:03,696[0m] {[34mscheduler_job.py:[0m588} INFO[0m - Executor reports execution of call_ptd.ptd_job run_id=scheduled__2023-10-26T18:00:00+00:00 exited with status success for try_number 1[0m
[[34m2023-10-27 16:31:03,721[0m] {[34mscheduler_job.py:[0m631} INFO[0m - TaskInstance Finished: dag_id=call_ptd, task_id=ptd_job, run_id=scheduled__2023-10-26T18:00:00+00:00, map_index=-1, run_start_date=2023-10-27 03:31:03.027217+00:00, run_end_date=2023-10-27 03:31:03.374728+00:00, run_duration=0.347511, state=failed, executor_state=success, try_number=1, max_tries=0, job_id=6, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-10-27 03:31:01.485145+00:00, queued_by_job_id=1, pid=3156293[0m
[[34m2023-10-27 16:31:04,185[0m] {[34mdagrun.py:[0m578} ERROR[0m - Marking run <DagRun call_ptd @ 2023-10-26 18:00:00+00:00: scheduled__2023-10-26T18:00:00+00:00, state:running, queued_at: 2023-10-27 03:31:01.274703+00:00. externally triggered: False> failed[0m
[[34m2023-10-27 16:31:04,186[0m] {[34mdagrun.py:[0m644} INFO[0m - DagRun Finished: dag_id=call_ptd, execution_date=2023-10-26 18:00:00+00:00, run_id=scheduled__2023-10-26T18:00:00+00:00, run_start_date=2023-10-27 03:31:01.342240+00:00, run_end_date=2023-10-27 03:31:04.186196+00:00, run_duration=2.843956, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2023-10-26 18:00:00+00:00, data_interval_end=2023-10-27 00:00:00+00:00, dag_hash=73809f00374e264528c5ff396bb50051[0m
[[34m2023-10-27 16:31:04,195[0m] {[34mdag.py:[0m3336} INFO[0m - Setting next_dagrun for call_ptd to 2023-10-27T00:00:00+00:00, run_after=2023-10-27T06:00:00+00:00[0m
[[34m2023-10-27 16:31:56,981[0m] {[34mscheduler_job.py:[0m346} INFO[0m - 1 tasks up for execution:
	<TaskInstance: call_ptd.ptd_job manual__2023-10-27T03:31:55.574845+00:00 [scheduled]>[0m
[[34m2023-10-27 16:31:56,982[0m] {[34mscheduler_job.py:[0m411} INFO[0m - DAG call_ptd has 0/16 running and queued tasks[0m
[[34m2023-10-27 16:31:56,983[0m] {[34mscheduler_job.py:[0m497} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: call_ptd.ptd_job manual__2023-10-27T03:31:55.574845+00:00 [scheduled]>[0m
[[34m2023-10-27 16:31:56,992[0m] {[34mscheduler_job.py:[0m536} INFO[0m - Sending TaskInstanceKey(dag_id='call_ptd', task_id='ptd_job', run_id='manual__2023-10-27T03:31:55.574845+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-10-27 16:31:56,992[0m] {[34mbase_executor.py:[0m95} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'call_ptd', 'ptd_job', 'manual__2023-10-27T03:31:55.574845+00:00', '--local', '--subdir', 'DAGS_FOLDER/ptd_airflow.py'][0m
[[34m2023-10-27 16:31:57,002[0m] {[34msequential_executor.py:[0m61} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'call_ptd', 'ptd_job', 'manual__2023-10-27T03:31:55.574845+00:00', '--local', '--subdir', 'DAGS_FOLDER/ptd_airflow.py'][0m
/home/zhangs/miniconda3/envs/ptd_esr_env/lib/python3.10/site-packages/airflow/models/base.py:49 MovedIn20Warning: Deprecated API features detected! These feature(s) are not compatible with SQLAlchemy 2.0. To prevent incompatible upgrades prior to updating applications, ensure requirements files are pinned to "sqlalchemy<2.0". Set environment variable SQLALCHEMY_WARN_20=1 to show all deprecation warnings.  Set environment variable SQLALCHEMY_SILENCE_UBER_WARNING=1 to silence this message. (Background on SQLAlchemy 2.0 at: https://sqlalche.me/e/b8d9)
[[34m2023-10-27 16:31:58,353[0m] {[34mdagbag.py:[0m537} INFO[0m - Filling up the DagBag from /home/zhangs/airflow/dags/ptd_airflow.py[0m
[[34m2023-10-27 16:31:58,875[0m] {[34mtask_command.py:[0m376} INFO[0m - Running <TaskInstance: call_ptd.ptd_job manual__2023-10-27T03:31:55.574845+00:00 [queued]> on host kscprod-data1.esr.cri.nz[0m
[[34m2023-10-27 16:32:03,675[0m] {[34mscheduler_job.py:[0m588} INFO[0m - Executor reports execution of call_ptd.ptd_job run_id=manual__2023-10-27T03:31:55.574845+00:00 exited with status success for try_number 1[0m
[[34m2023-10-27 16:32:03,702[0m] {[34mscheduler_job.py:[0m631} INFO[0m - TaskInstance Finished: dag_id=call_ptd, task_id=ptd_job, run_id=manual__2023-10-27T03:31:55.574845+00:00, map_index=-1, run_start_date=2023-10-27 03:31:58.982928+00:00, run_end_date=2023-10-27 03:32:03.225405+00:00, run_duration=4.242477, state=success, executor_state=success, try_number=1, max_tries=0, job_id=7, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-10-27 03:31:56.984962+00:00, queued_by_job_id=1, pid=3186698[0m
[[34m2023-10-27 16:32:04,479[0m] {[34mdagrun.py:[0m597} INFO[0m - Marking run <DagRun call_ptd @ 2023-10-27 03:31:55.574845+00:00: manual__2023-10-27T03:31:55.574845+00:00, state:running, queued_at: 2023-10-27 03:31:55.639234+00:00. externally triggered: True> successful[0m
[[34m2023-10-27 16:32:04,480[0m] {[34mdagrun.py:[0m644} INFO[0m - DagRun Finished: dag_id=call_ptd, execution_date=2023-10-27 03:31:55.574845+00:00, run_id=manual__2023-10-27T03:31:55.574845+00:00, run_start_date=2023-10-27 03:31:56.802714+00:00, run_end_date=2023-10-27 03:32:04.480190+00:00, run_duration=7.677476, state=success, external_trigger=True, run_type=manual, data_interval_start=2023-10-26 21:31:55.574845+00:00, data_interval_end=2023-10-27 03:31:55.574845+00:00, dag_hash=73809f00374e264528c5ff396bb50051[0m
[[34m2023-10-27 16:32:04,489[0m] {[34mdag.py:[0m3336} INFO[0m - Setting next_dagrun for call_ptd to 2023-10-27T03:31:55.574845+00:00, run_after=2023-10-27T09:31:55.574845+00:00[0m
[[34m2023-10-27 16:34:03,582[0m] {[34mscheduler_job.py:[0m1380} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2023-10-27 16:39:04,226[0m] {[34mscheduler_job.py:[0m1380} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2023-10-27 16:44:04,784[0m] {[34mscheduler_job.py:[0m1380} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2023-10-27 16:49:05,210[0m] {[34mscheduler_job.py:[0m1380} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2023-10-27 16:54:05,662[0m] {[34mscheduler_job.py:[0m1380} INFO[0m - Resetting orphaned tasks for active dag runs[0m
/home/zhangs/miniconda3/envs/airflow/lib/python3.9/site-packages/airflow/models/base.py:49 MovedIn20Warning: Deprecated API features detected! These feature(s) are not compatible with SQLAlchemy 2.0. To prevent incompatible upgrades prior to updating applications, ensure requirements files are pinned to "sqlalchemy<2.0". Set environment variable SQLALCHEMY_WARN_20=1 to show all deprecation warnings.  Set environment variable SQLALCHEMY_SILENCE_UBER_WARNING=1 to silence this message. (Background on SQLAlchemy 2.0 at: https://sqlalche.me/e/b8d9)
  ____________       _____________
 ____    |__( )_________  __/__  /________      __
____  /| |_  /__  ___/_  /_ __  /_  __ \_ | /| / /
___  ___ |  / _  /   _  __/ _  / / /_/ /_ |/ |/ /
 _/_/  |_/_/  /_/    /_/    /_/  \____/____/|__/
[2023-10-27 16:57:54 +1300] [4177758] [INFO] Starting gunicorn 21.2.0
[2023-10-27 16:57:54 +1300] [4177758] [INFO] Listening at: http://[::]:8083 (4177758)
[2023-10-27 16:57:54 +1300] [4177758] [INFO] Using worker: sync
[2023-10-27 16:57:54 +1300] [4177778] [INFO] Booting worker with pid: 4177778
[[34m2023-10-27 16:57:54,283[0m] {[34mscheduler_job.py:[0m700} INFO[0m - Starting the scheduler[0m
[[34m2023-10-27 16:57:54,284[0m] {[34mscheduler_job.py:[0m705} INFO[0m - Processing each file at most -1 times[0m
[[34m2023-10-27 16:57:54,287[0m] {[34mexecutor_loader.py:[0m107} INFO[0m - Loaded executor: SequentialExecutor[0m
[[34m2023-10-27 16:57:54,293[0m] {[34mmanager.py:[0m163} INFO[0m - Launched DagFileProcessorManager with pid: 4177793[0m
[[34m2023-10-27 16:57:54,294[0m] {[34mscheduler_job.py:[0m1380} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2023-10-27 16:57:54,300[0m] {[34msettings.py:[0m58} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2023-10-27T16:57:54.317+1300] {manager.py:409} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2) when using sqlite. So we set parallelism to 1.
[2023-10-27 16:57:54 +1300] [4177816] [INFO] Booting worker with pid: 4177816
[[34m2023-10-27 16:58:16,957[0m] {[34mdag.py:[0m3336} INFO[0m - Setting next_dagrun for call_ptd to 2023-10-26T06:00:00+00:00, run_after=2023-10-26T12:00:00+00:00[0m
[[34m2023-10-27 16:58:17,357[0m] {[34mscheduler_job.py:[0m346} INFO[0m - 2 tasks up for execution:
	<TaskInstance: call_ptd.ptd_job scheduled__2023-10-26T00:00:00+00:00 [scheduled]>
	<TaskInstance: call_ptd.ptd_job manual__2023-10-27T03:58:16.398461+00:00 [scheduled]>[0m
[[34m2023-10-27 16:58:17,358[0m] {[34mscheduler_job.py:[0m411} INFO[0m - DAG call_ptd has 0/16 running and queued tasks[0m
[[34m2023-10-27 16:58:17,358[0m] {[34mscheduler_job.py:[0m411} INFO[0m - DAG call_ptd has 1/16 running and queued tasks[0m
[[34m2023-10-27 16:58:17,358[0m] {[34mscheduler_job.py:[0m497} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: call_ptd.ptd_job scheduled__2023-10-26T00:00:00+00:00 [scheduled]>
	<TaskInstance: call_ptd.ptd_job manual__2023-10-27T03:58:16.398461+00:00 [scheduled]>[0m
[[34m2023-10-27 16:58:17,387[0m] {[34mscheduler_job.py:[0m536} INFO[0m - Sending TaskInstanceKey(dag_id='call_ptd', task_id='ptd_job', run_id='scheduled__2023-10-26T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-10-27 16:58:17,387[0m] {[34mbase_executor.py:[0m95} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'call_ptd', 'ptd_job', 'scheduled__2023-10-26T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/ptd_airflow.py'][0m
[[34m2023-10-27 16:58:17,388[0m] {[34mscheduler_job.py:[0m536} INFO[0m - Sending TaskInstanceKey(dag_id='call_ptd', task_id='ptd_job', run_id='manual__2023-10-27T03:58:16.398461+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-10-27 16:58:17,389[0m] {[34mbase_executor.py:[0m95} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'call_ptd', 'ptd_job', 'manual__2023-10-27T03:58:16.398461+00:00', '--local', '--subdir', 'DAGS_FOLDER/ptd_airflow.py'][0m
[[34m2023-10-27 16:58:17,402[0m] {[34msequential_executor.py:[0m61} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'call_ptd', 'ptd_job', 'scheduled__2023-10-26T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/ptd_airflow.py'][0m
[[34m2023-10-27 16:58:17,420[0m] {[34mscheduler_job.py:[0m762} ERROR[0m - Exception when executing SchedulerJob._run_scheduler_loop[0m
Traceback (most recent call last):
  File "/home/zhangs/miniconda3/envs/airflow/lib/python3.9/site-packages/airflow/jobs/scheduler_job.py", line 745, in _execute
    self._run_scheduler_loop()
  File "/home/zhangs/miniconda3/envs/airflow/lib/python3.9/site-packages/airflow/jobs/scheduler_job.py", line 867, in _run_scheduler_loop
    self.executor.heartbeat()
  File "/home/zhangs/miniconda3/envs/airflow/lib/python3.9/site-packages/airflow/executors/base_executor.py", line 175, in heartbeat
    self.sync()
  File "/home/zhangs/miniconda3/envs/airflow/lib/python3.9/site-packages/airflow/executors/sequential_executor.py", line 64, in sync
    subprocess.check_call(command, close_fds=True)
  File "/home/zhangs/miniconda3/envs/airflow/lib/python3.9/subprocess.py", line 368, in check_call
    retcode = call(*popenargs, **kwargs)
  File "/home/zhangs/miniconda3/envs/airflow/lib/python3.9/subprocess.py", line 349, in call
    with Popen(*popenargs, **kwargs) as p:
  File "/home/zhangs/miniconda3/envs/airflow/lib/python3.9/subprocess.py", line 951, in __init__
    self._execute_child(args, executable, preexec_fn, close_fds,
  File "/home/zhangs/miniconda3/envs/airflow/lib/python3.9/subprocess.py", line 1837, in _execute_child
    raise child_exception_type(errno_num, err_msg, err_filename)
FileNotFoundError: [Errno 2] No such file or directory: 'airflow'
[[34m2023-10-27 16:58:17,427[0m] {[34msequential_executor.py:[0m61} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'call_ptd', 'ptd_job', 'scheduled__2023-10-26T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/ptd_airflow.py'][0m
[[34m2023-10-27 16:58:17,438[0m] {[34mscheduler_job.py:[0m768} ERROR[0m - Exception when executing Executor.end[0m
Traceback (most recent call last):
  File "/home/zhangs/miniconda3/envs/airflow/lib/python3.9/site-packages/airflow/jobs/scheduler_job.py", line 745, in _execute
    self._run_scheduler_loop()
  File "/home/zhangs/miniconda3/envs/airflow/lib/python3.9/site-packages/airflow/jobs/scheduler_job.py", line 867, in _run_scheduler_loop
    self.executor.heartbeat()
  File "/home/zhangs/miniconda3/envs/airflow/lib/python3.9/site-packages/airflow/executors/base_executor.py", line 175, in heartbeat
    self.sync()
  File "/home/zhangs/miniconda3/envs/airflow/lib/python3.9/site-packages/airflow/executors/sequential_executor.py", line 64, in sync
    subprocess.check_call(command, close_fds=True)
  File "/home/zhangs/miniconda3/envs/airflow/lib/python3.9/subprocess.py", line 368, in check_call
    retcode = call(*popenargs, **kwargs)
  File "/home/zhangs/miniconda3/envs/airflow/lib/python3.9/subprocess.py", line 349, in call
    with Popen(*popenargs, **kwargs) as p:
  File "/home/zhangs/miniconda3/envs/airflow/lib/python3.9/subprocess.py", line 951, in __init__
    self._execute_child(args, executable, preexec_fn, close_fds,
  File "/home/zhangs/miniconda3/envs/airflow/lib/python3.9/subprocess.py", line 1837, in _execute_child
    raise child_exception_type(errno_num, err_msg, err_filename)
FileNotFoundError: [Errno 2] No such file or directory: 'airflow'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/zhangs/miniconda3/envs/airflow/lib/python3.9/site-packages/airflow/jobs/scheduler_job.py", line 766, in _execute
    self.executor.end()
  File "/home/zhangs/miniconda3/envs/airflow/lib/python3.9/site-packages/airflow/executors/sequential_executor.py", line 74, in end
    self.heartbeat()
  File "/home/zhangs/miniconda3/envs/airflow/lib/python3.9/site-packages/airflow/executors/base_executor.py", line 175, in heartbeat
    self.sync()
  File "/home/zhangs/miniconda3/envs/airflow/lib/python3.9/site-packages/airflow/executors/sequential_executor.py", line 64, in sync
    subprocess.check_call(command, close_fds=True)
  File "/home/zhangs/miniconda3/envs/airflow/lib/python3.9/subprocess.py", line 368, in check_call
    retcode = call(*popenargs, **kwargs)
  File "/home/zhangs/miniconda3/envs/airflow/lib/python3.9/subprocess.py", line 349, in call
    with Popen(*popenargs, **kwargs) as p:
  File "/home/zhangs/miniconda3/envs/airflow/lib/python3.9/subprocess.py", line 951, in __init__
    self._execute_child(args, executable, preexec_fn, close_fds,
  File "/home/zhangs/miniconda3/envs/airflow/lib/python3.9/subprocess.py", line 1837, in _execute_child
    raise child_exception_type(errno_num, err_msg, err_filename)
FileNotFoundError: [Errno 2] No such file or directory: 'airflow'
[[34m2023-10-27 16:58:18,482[0m] {[34mprocess_utils.py:[0m129} INFO[0m - Sending Signals.SIGTERM to group 4177793. PIDs of all processes in the group: [4177793][0m
[[34m2023-10-27 16:58:18,482[0m] {[34mprocess_utils.py:[0m84} INFO[0m - Sending the signal Signals.SIGTERM to group 4177793[0m
[[34m2023-10-27 16:58:18,855[0m] {[34mprocess_utils.py:[0m79} INFO[0m - Process psutil.Process(pid=4177793, status='terminated', exitcode=0, started='16:57:53') (4177793) terminated with exit code 0[0m
[[34m2023-10-27 16:58:18,856[0m] {[34mscheduler_job.py:[0m774} INFO[0m - Exited execute loop[0m
[2023-10-27 16:58:18 +1300] [4177758] [INFO] Handling signal: term
[2023-10-27 16:58:18 +1300] [4177778] [INFO] Worker exiting (pid: 4177778)
[2023-10-27 16:58:18 +1300] [4177816] [INFO] Worker exiting (pid: 4177816)
Traceback (most recent call last):
  File "/home/zhangs/miniconda3/envs/airflow/bin/airflow", line 11, in <module>
    sys.exit(main())
  File "/home/zhangs/miniconda3/envs/airflow/lib/python3.9/site-packages/airflow/__main__.py", line 39, in main
    args.func(args)
  File "/home/zhangs/miniconda3/envs/airflow/lib/python3.9/site-packages/airflow/cli/cli_parser.py", line 52, in command
    return func(*args, **kwargs)
  File "/home/zhangs/miniconda3/envs/airflow/lib/python3.9/site-packages/airflow/utils/cli.py", line 103, in wrapper
    return f(*args, **kwargs)
  File "/home/zhangs/miniconda3/envs/airflow/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 85, in scheduler
    _run_scheduler_job(args=args)
  File "/home/zhangs/miniconda3/envs/airflow/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 50, in _run_scheduler_job
    job.run()
  File "/home/zhangs/miniconda3/envs/airflow/lib/python3.9/site-packages/airflow/jobs/base_job.py", line 247, in run
    self._execute()
  File "/home/zhangs/miniconda3/envs/airflow/lib/python3.9/site-packages/airflow/jobs/scheduler_job.py", line 745, in _execute
    self._run_scheduler_loop()
  File "/home/zhangs/miniconda3/envs/airflow/lib/python3.9/site-packages/airflow/jobs/scheduler_job.py", line 867, in _run_scheduler_loop
    self.executor.heartbeat()
  File "/home/zhangs/miniconda3/envs/airflow/lib/python3.9/site-packages/airflow/executors/base_executor.py", line 175, in heartbeat
    self.sync()
  File "/home/zhangs/miniconda3/envs/airflow/lib/python3.9/site-packages/airflow/executors/sequential_executor.py", line 64, in sync
    subprocess.check_call(command, close_fds=True)
  File "/home/zhangs/miniconda3/envs/airflow/lib/python3.9/subprocess.py", line 368, in check_call
    retcode = call(*popenargs, **kwargs)
  File "/home/zhangs/miniconda3/envs/airflow/lib/python3.9/subprocess.py", line 349, in call
    with Popen(*popenargs, **kwargs) as p:
  File "/home/zhangs/miniconda3/envs/airflow/lib/python3.9/subprocess.py", line 951, in __init__
    self._execute_child(args, executable, preexec_fn, close_fds,
  File "/home/zhangs/miniconda3/envs/airflow/lib/python3.9/subprocess.py", line 1837, in _execute_child
    raise child_exception_type(errno_num, err_msg, err_filename)
FileNotFoundError: [Errno 2] No such file or directory: 'airflow'
[2023-10-27 16:58:18 +1300] [4177758] [INFO] Shutting down: Master
/home/zhangs/miniconda3/envs/airflow/lib/python3.9/site-packages/airflow/models/base.py:49 MovedIn20Warning: Deprecated API features detected! These feature(s) are not compatible with SQLAlchemy 2.0. To prevent incompatible upgrades prior to updating applications, ensure requirements files are pinned to "sqlalchemy<2.0". Set environment variable SQLALCHEMY_WARN_20=1 to show all deprecation warnings.  Set environment variable SQLALCHEMY_SILENCE_UBER_WARNING=1 to silence this message. (Background on SQLAlchemy 2.0 at: https://sqlalche.me/e/b8d9)
  ____________       _____________
 ____    |__( )_________  __/__  /________      __
____  /| |_  /__  ___/_  /_ __  /_  __ \_ | /| / /
___  ___ |  / _  /   _  __/ _  / / /_/ /_ |/ |/ /
 _/_/  |_/_/  /_/    /_/    /_/  \____/____/|__/
[2023-10-27 17:10:53 +1300] [432344] [INFO] Starting gunicorn 21.2.0
[2023-10-27 17:10:53 +1300] [432344] [INFO] Listening at: http://[::]:8083 (432344)
[2023-10-27 17:10:53 +1300] [432344] [INFO] Using worker: sync
[2023-10-27 17:10:53 +1300] [432367] [INFO] Booting worker with pid: 432367
[[34m2023-10-27 17:10:53,304[0m] {[34mscheduler_job.py:[0m700} INFO[0m - Starting the scheduler[0m
[[34m2023-10-27 17:10:53,305[0m] {[34mscheduler_job.py:[0m705} INFO[0m - Processing each file at most -1 times[0m
[[34m2023-10-27 17:10:53,310[0m] {[34mexecutor_loader.py:[0m107} INFO[0m - Loaded executor: SequentialExecutor[0m
[[34m2023-10-27 17:10:53,316[0m] {[34mmanager.py:[0m163} INFO[0m - Launched DagFileProcessorManager with pid: 432379[0m
[[34m2023-10-27 17:10:53,318[0m] {[34mscheduler_job.py:[0m1380} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[2023-10-27 17:10:53 +1300] [432376] [INFO] Booting worker with pid: 432376
[[34m2023-10-27 17:10:53,328[0m] {[34msettings.py:[0m58} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2023-10-27T17:10:53.342+1300] {manager.py:409} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2) when using sqlite. So we set parallelism to 1.
[[34m2023-10-27 17:11:17,621[0m] {[34mdag.py:[0m3336} INFO[0m - Setting next_dagrun for call_ptd to 2023-10-26T06:00:00+00:00, run_after=2023-10-26T12:00:00+00:00[0m
[[34m2023-10-27 17:11:17,982[0m] {[34mscheduler_job.py:[0m346} INFO[0m - 2 tasks up for execution:
	<TaskInstance: call_ptd.ptd_job scheduled__2023-10-26T00:00:00+00:00 [scheduled]>
	<TaskInstance: call_ptd.ptd_job manual__2023-10-27T04:11:15.928713+00:00 [scheduled]>[0m
[[34m2023-10-27 17:11:17,983[0m] {[34mscheduler_job.py:[0m411} INFO[0m - DAG call_ptd has 0/16 running and queued tasks[0m
[[34m2023-10-27 17:11:17,983[0m] {[34mscheduler_job.py:[0m411} INFO[0m - DAG call_ptd has 1/16 running and queued tasks[0m
[[34m2023-10-27 17:11:17,983[0m] {[34mscheduler_job.py:[0m497} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: call_ptd.ptd_job scheduled__2023-10-26T00:00:00+00:00 [scheduled]>
	<TaskInstance: call_ptd.ptd_job manual__2023-10-27T04:11:15.928713+00:00 [scheduled]>[0m
[[34m2023-10-27 17:11:17,992[0m] {[34mscheduler_job.py:[0m536} INFO[0m - Sending TaskInstanceKey(dag_id='call_ptd', task_id='ptd_job', run_id='scheduled__2023-10-26T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-10-27 17:11:17,992[0m] {[34mbase_executor.py:[0m95} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'call_ptd', 'ptd_job', 'scheduled__2023-10-26T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/ptd_airflow.py'][0m
[[34m2023-10-27 17:11:17,993[0m] {[34mscheduler_job.py:[0m536} INFO[0m - Sending TaskInstanceKey(dag_id='call_ptd', task_id='ptd_job', run_id='manual__2023-10-27T04:11:15.928713+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-10-27 17:11:17,994[0m] {[34mbase_executor.py:[0m95} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'call_ptd', 'ptd_job', 'manual__2023-10-27T04:11:15.928713+00:00', '--local', '--subdir', 'DAGS_FOLDER/ptd_airflow.py'][0m
[[34m2023-10-27 17:11:18,003[0m] {[34msequential_executor.py:[0m61} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'call_ptd', 'ptd_job', 'scheduled__2023-10-26T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/ptd_airflow.py'][0m
/home/zhangs/miniconda3/envs/ptd_esr_env/lib/python3.10/site-packages/airflow/models/base.py:49 MovedIn20Warning: Deprecated API features detected! These feature(s) are not compatible with SQLAlchemy 2.0. To prevent incompatible upgrades prior to updating applications, ensure requirements files are pinned to "sqlalchemy<2.0". Set environment variable SQLALCHEMY_WARN_20=1 to show all deprecation warnings.  Set environment variable SQLALCHEMY_SILENCE_UBER_WARNING=1 to silence this message. (Background on SQLAlchemy 2.0 at: https://sqlalche.me/e/b8d9)
[[34m2023-10-27 17:11:19,980[0m] {[34mdagbag.py:[0m537} INFO[0m - Filling up the DagBag from /home/zhangs/airflow/dags/ptd_airflow.py[0m
[[34m2023-10-27 17:11:20,672[0m] {[34mtask_command.py:[0m376} INFO[0m - Running <TaskInstance: call_ptd.ptd_job scheduled__2023-10-26T00:00:00+00:00 [queued]> on host kscprod-data1.esr.cri.nz[0m
[[34m2023-10-27 17:11:28,502[0m] {[34msequential_executor.py:[0m61} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'call_ptd', 'ptd_job', 'manual__2023-10-27T04:11:15.928713+00:00', '--local', '--subdir', 'DAGS_FOLDER/ptd_airflow.py'][0m
/home/zhangs/miniconda3/envs/ptd_esr_env/lib/python3.10/site-packages/airflow/models/base.py:49 MovedIn20Warning: Deprecated API features detected! These feature(s) are not compatible with SQLAlchemy 2.0. To prevent incompatible upgrades prior to updating applications, ensure requirements files are pinned to "sqlalchemy<2.0". Set environment variable SQLALCHEMY_WARN_20=1 to show all deprecation warnings.  Set environment variable SQLALCHEMY_SILENCE_UBER_WARNING=1 to silence this message. (Background on SQLAlchemy 2.0 at: https://sqlalche.me/e/b8d9)
[[34m2023-10-27 17:11:29,538[0m] {[34mdagbag.py:[0m537} INFO[0m - Filling up the DagBag from /home/zhangs/airflow/dags/ptd_airflow.py[0m
[[34m2023-10-27 17:11:29,979[0m] {[34mtask_command.py:[0m376} INFO[0m - Running <TaskInstance: call_ptd.ptd_job manual__2023-10-27T04:11:15.928713+00:00 [queued]> on host kscprod-data1.esr.cri.nz[0m
[[34m2023-10-27 17:11:36,799[0m] {[34mscheduler_job.py:[0m588} INFO[0m - Executor reports execution of call_ptd.ptd_job run_id=scheduled__2023-10-26T00:00:00+00:00 exited with status success for try_number 1[0m
[[34m2023-10-27 17:11:36,800[0m] {[34mscheduler_job.py:[0m588} INFO[0m - Executor reports execution of call_ptd.ptd_job run_id=manual__2023-10-27T04:11:15.928713+00:00 exited with status success for try_number 1[0m
[[34m2023-10-27 17:11:36,836[0m] {[34mscheduler_job.py:[0m631} INFO[0m - TaskInstance Finished: dag_id=call_ptd, task_id=ptd_job, run_id=manual__2023-10-27T04:11:15.928713+00:00, map_index=-1, run_start_date=2023-10-27 04:11:30.078345+00:00, run_end_date=2023-10-27 04:11:36.509671+00:00, run_duration=6.431326, state=success, executor_state=success, try_number=1, max_tries=0, job_id=3, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-10-27 04:11:17.984667+00:00, queued_by_job_id=1, pid=453689[0m
[[34m2023-10-27 17:11:36,836[0m] {[34mscheduler_job.py:[0m631} INFO[0m - TaskInstance Finished: dag_id=call_ptd, task_id=ptd_job, run_id=scheduled__2023-10-26T00:00:00+00:00, map_index=-1, run_start_date=2023-10-27 04:11:20.805370+00:00, run_end_date=2023-10-27 04:11:28.176423+00:00, run_duration=7.371053, state=success, executor_state=success, try_number=1, max_tries=0, job_id=2, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-10-27 04:11:17.984667+00:00, queued_by_job_id=1, pid=447926[0m
[[34m2023-10-27 17:11:37,337[0m] {[34mdag.py:[0m3336} INFO[0m - Setting next_dagrun for call_ptd to 2023-10-26T12:00:00+00:00, run_after=2023-10-26T18:00:00+00:00[0m
[[34m2023-10-27 17:11:37,450[0m] {[34mdagrun.py:[0m597} INFO[0m - Marking run <DagRun call_ptd @ 2023-10-26 00:00:00+00:00: scheduled__2023-10-26T00:00:00+00:00, state:running, queued_at: 2023-10-27 04:11:17.602836+00:00. externally triggered: False> successful[0m
[[34m2023-10-27 17:11:37,451[0m] {[34mdagrun.py:[0m644} INFO[0m - DagRun Finished: dag_id=call_ptd, execution_date=2023-10-26 00:00:00+00:00, run_id=scheduled__2023-10-26T00:00:00+00:00, run_start_date=2023-10-27 04:11:17.664520+00:00, run_end_date=2023-10-27 04:11:37.451642+00:00, run_duration=19.787122, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-10-26 00:00:00+00:00, data_interval_end=2023-10-26 06:00:00+00:00, dag_hash=73809f00374e264528c5ff396bb50051[0m
[[34m2023-10-27 17:11:37,456[0m] {[34mdag.py:[0m3336} INFO[0m - Setting next_dagrun for call_ptd to 2023-10-26T06:00:00+00:00, run_after=2023-10-26T12:00:00+00:00[0m
[[34m2023-10-27 17:11:37,458[0m] {[34mdagrun.py:[0m597} INFO[0m - Marking run <DagRun call_ptd @ 2023-10-27 04:11:15.928713+00:00: manual__2023-10-27T04:11:15.928713+00:00, state:running, queued_at: 2023-10-27 04:11:16.047615+00:00. externally triggered: True> successful[0m
[[34m2023-10-27 17:11:37,458[0m] {[34mdagrun.py:[0m644} INFO[0m - DagRun Finished: dag_id=call_ptd, execution_date=2023-10-27 04:11:15.928713+00:00, run_id=manual__2023-10-27T04:11:15.928713+00:00, run_start_date=2023-10-27 04:11:17.665060+00:00, run_end_date=2023-10-27 04:11:37.458766+00:00, run_duration=19.793706, state=success, external_trigger=True, run_type=manual, data_interval_start=2023-10-26 22:11:15.928713+00:00, data_interval_end=2023-10-27 04:11:15.928713+00:00, dag_hash=73809f00374e264528c5ff396bb50051[0m
[[34m2023-10-27 17:11:37,461[0m] {[34mdag.py:[0m3336} INFO[0m - Setting next_dagrun for call_ptd to 2023-10-27T04:11:15.928713+00:00, run_after=2023-10-27T10:11:15.928713+00:00[0m
[[34m2023-10-27 17:11:37,523[0m] {[34mscheduler_job.py:[0m346} INFO[0m - 1 tasks up for execution:
	<TaskInstance: call_ptd.ptd_job scheduled__2023-10-26T06:00:00+00:00 [scheduled]>[0m
[[34m2023-10-27 17:11:37,523[0m] {[34mscheduler_job.py:[0m411} INFO[0m - DAG call_ptd has 0/16 running and queued tasks[0m
[[34m2023-10-27 17:11:37,523[0m] {[34mscheduler_job.py:[0m497} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: call_ptd.ptd_job scheduled__2023-10-26T06:00:00+00:00 [scheduled]>[0m
[[34m2023-10-27 17:11:37,533[0m] {[34mscheduler_job.py:[0m536} INFO[0m - Sending TaskInstanceKey(dag_id='call_ptd', task_id='ptd_job', run_id='scheduled__2023-10-26T06:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-10-27 17:11:37,534[0m] {[34mbase_executor.py:[0m95} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'call_ptd', 'ptd_job', 'scheduled__2023-10-26T06:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/ptd_airflow.py'][0m
[[34m2023-10-27 17:11:37,546[0m] {[34msequential_executor.py:[0m61} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'call_ptd', 'ptd_job', 'scheduled__2023-10-26T06:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/ptd_airflow.py'][0m
/home/zhangs/miniconda3/envs/ptd_esr_env/lib/python3.10/site-packages/airflow/models/base.py:49 MovedIn20Warning: Deprecated API features detected! These feature(s) are not compatible with SQLAlchemy 2.0. To prevent incompatible upgrades prior to updating applications, ensure requirements files are pinned to "sqlalchemy<2.0". Set environment variable SQLALCHEMY_WARN_20=1 to show all deprecation warnings.  Set environment variable SQLALCHEMY_SILENCE_UBER_WARNING=1 to silence this message. (Background on SQLAlchemy 2.0 at: https://sqlalche.me/e/b8d9)
[[34m2023-10-27 17:11:38,798[0m] {[34mdagbag.py:[0m537} INFO[0m - Filling up the DagBag from /home/zhangs/airflow/dags/ptd_airflow.py[0m
[[34m2023-10-27 17:11:39,269[0m] {[34mtask_command.py:[0m376} INFO[0m - Running <TaskInstance: call_ptd.ptd_job scheduled__2023-10-26T06:00:00+00:00 [queued]> on host kscprod-data1.esr.cri.nz[0m
[[34m2023-10-27 17:11:46,621[0m] {[34mscheduler_job.py:[0m588} INFO[0m - Executor reports execution of call_ptd.ptd_job run_id=scheduled__2023-10-26T06:00:00+00:00 exited with status success for try_number 1[0m
[[34m2023-10-27 17:11:46,654[0m] {[34mscheduler_job.py:[0m631} INFO[0m - TaskInstance Finished: dag_id=call_ptd, task_id=ptd_job, run_id=scheduled__2023-10-26T06:00:00+00:00, map_index=-1, run_start_date=2023-10-27 04:11:39.390192+00:00, run_end_date=2023-10-27 04:11:46.108980+00:00, run_duration=6.718788, state=success, executor_state=success, try_number=1, max_tries=0, job_id=4, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-10-27 04:11:37.524977+00:00, queued_by_job_id=1, pid=457239[0m
[[34m2023-10-27 17:11:47,116[0m] {[34mdag.py:[0m3336} INFO[0m - Setting next_dagrun for call_ptd to 2023-10-26T18:00:00+00:00, run_after=2023-10-27T00:00:00+00:00[0m
[[34m2023-10-27 17:11:47,227[0m] {[34mdagrun.py:[0m597} INFO[0m - Marking run <DagRun call_ptd @ 2023-10-26 06:00:00+00:00: scheduled__2023-10-26T06:00:00+00:00, state:running, queued_at: 2023-10-27 04:11:37.323640+00:00. externally triggered: False> successful[0m
[[34m2023-10-27 17:11:47,227[0m] {[34mdagrun.py:[0m644} INFO[0m - DagRun Finished: dag_id=call_ptd, execution_date=2023-10-26 06:00:00+00:00, run_id=scheduled__2023-10-26T06:00:00+00:00, run_start_date=2023-10-27 04:11:37.380953+00:00, run_end_date=2023-10-27 04:11:47.227676+00:00, run_duration=9.846723, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-10-26 06:00:00+00:00, data_interval_end=2023-10-26 12:00:00+00:00, dag_hash=73809f00374e264528c5ff396bb50051[0m
[[34m2023-10-27 17:11:47,232[0m] {[34mdag.py:[0m3336} INFO[0m - Setting next_dagrun for call_ptd to 2023-10-26T12:00:00+00:00, run_after=2023-10-26T18:00:00+00:00[0m
[[34m2023-10-27 17:11:47,290[0m] {[34mscheduler_job.py:[0m346} INFO[0m - 1 tasks up for execution:
	<TaskInstance: call_ptd.ptd_job scheduled__2023-10-26T12:00:00+00:00 [scheduled]>[0m
[[34m2023-10-27 17:11:47,290[0m] {[34mscheduler_job.py:[0m411} INFO[0m - DAG call_ptd has 0/16 running and queued tasks[0m
[[34m2023-10-27 17:11:47,290[0m] {[34mscheduler_job.py:[0m497} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: call_ptd.ptd_job scheduled__2023-10-26T12:00:00+00:00 [scheduled]>[0m
[[34m2023-10-27 17:11:47,298[0m] {[34mscheduler_job.py:[0m536} INFO[0m - Sending TaskInstanceKey(dag_id='call_ptd', task_id='ptd_job', run_id='scheduled__2023-10-26T12:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-10-27 17:11:47,298[0m] {[34mbase_executor.py:[0m95} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'call_ptd', 'ptd_job', 'scheduled__2023-10-26T12:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/ptd_airflow.py'][0m
[[34m2023-10-27 17:11:47,308[0m] {[34msequential_executor.py:[0m61} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'call_ptd', 'ptd_job', 'scheduled__2023-10-26T12:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/ptd_airflow.py'][0m
/home/zhangs/miniconda3/envs/ptd_esr_env/lib/python3.10/site-packages/airflow/models/base.py:49 MovedIn20Warning: Deprecated API features detected! These feature(s) are not compatible with SQLAlchemy 2.0. To prevent incompatible upgrades prior to updating applications, ensure requirements files are pinned to "sqlalchemy<2.0". Set environment variable SQLALCHEMY_WARN_20=1 to show all deprecation warnings.  Set environment variable SQLALCHEMY_SILENCE_UBER_WARNING=1 to silence this message. (Background on SQLAlchemy 2.0 at: https://sqlalche.me/e/b8d9)
[[34m2023-10-27 17:11:48,473[0m] {[34mdagbag.py:[0m537} INFO[0m - Filling up the DagBag from /home/zhangs/airflow/dags/ptd_airflow.py[0m
[[34m2023-10-27 17:11:48,942[0m] {[34mtask_command.py:[0m376} INFO[0m - Running <TaskInstance: call_ptd.ptd_job scheduled__2023-10-26T12:00:00+00:00 [queued]> on host kscprod-data1.esr.cri.nz[0m
[[34m2023-10-27 17:11:55,877[0m] {[34mscheduler_job.py:[0m588} INFO[0m - Executor reports execution of call_ptd.ptd_job run_id=scheduled__2023-10-26T12:00:00+00:00 exited with status success for try_number 1[0m
[[34m2023-10-27 17:11:55,904[0m] {[34mscheduler_job.py:[0m631} INFO[0m - TaskInstance Finished: dag_id=call_ptd, task_id=ptd_job, run_id=scheduled__2023-10-26T12:00:00+00:00, map_index=-1, run_start_date=2023-10-27 04:11:49.051380+00:00, run_end_date=2023-10-27 04:11:55.568386+00:00, run_duration=6.517006, state=success, executor_state=success, try_number=1, max_tries=0, job_id=5, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-10-27 04:11:47.291857+00:00, queued_by_job_id=1, pid=461870[0m
[[34m2023-10-27 17:11:56,419[0m] {[34mdag.py:[0m3336} INFO[0m - Setting next_dagrun for call_ptd to 2023-10-27T00:00:00+00:00, run_after=2023-10-27T06:00:00+00:00[0m
[[34m2023-10-27 17:11:56,531[0m] {[34mdagrun.py:[0m597} INFO[0m - Marking run <DagRun call_ptd @ 2023-10-26 12:00:00+00:00: scheduled__2023-10-26T12:00:00+00:00, state:running, queued_at: 2023-10-27 04:11:47.102395+00:00. externally triggered: False> successful[0m
[[34m2023-10-27 17:11:56,531[0m] {[34mdagrun.py:[0m644} INFO[0m - DagRun Finished: dag_id=call_ptd, execution_date=2023-10-26 12:00:00+00:00, run_id=scheduled__2023-10-26T12:00:00+00:00, run_start_date=2023-10-27 04:11:47.159020+00:00, run_end_date=2023-10-27 04:11:56.531743+00:00, run_duration=9.372723, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-10-26 12:00:00+00:00, data_interval_end=2023-10-26 18:00:00+00:00, dag_hash=73809f00374e264528c5ff396bb50051[0m
[[34m2023-10-27 17:11:56,535[0m] {[34mdag.py:[0m3336} INFO[0m - Setting next_dagrun for call_ptd to 2023-10-26T18:00:00+00:00, run_after=2023-10-27T00:00:00+00:00[0m
[[34m2023-10-27 17:11:56,592[0m] {[34mscheduler_job.py:[0m346} INFO[0m - 1 tasks up for execution:
	<TaskInstance: call_ptd.ptd_job scheduled__2023-10-26T18:00:00+00:00 [scheduled]>[0m
[[34m2023-10-27 17:11:56,593[0m] {[34mscheduler_job.py:[0m411} INFO[0m - DAG call_ptd has 0/16 running and queued tasks[0m
[[34m2023-10-27 17:11:56,593[0m] {[34mscheduler_job.py:[0m497} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: call_ptd.ptd_job scheduled__2023-10-26T18:00:00+00:00 [scheduled]>[0m
[[34m2023-10-27 17:11:56,601[0m] {[34mscheduler_job.py:[0m536} INFO[0m - Sending TaskInstanceKey(dag_id='call_ptd', task_id='ptd_job', run_id='scheduled__2023-10-26T18:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-10-27 17:11:56,601[0m] {[34mbase_executor.py:[0m95} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'call_ptd', 'ptd_job', 'scheduled__2023-10-26T18:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/ptd_airflow.py'][0m
[[34m2023-10-27 17:11:56,611[0m] {[34msequential_executor.py:[0m61} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'call_ptd', 'ptd_job', 'scheduled__2023-10-26T18:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/ptd_airflow.py'][0m
/home/zhangs/miniconda3/envs/ptd_esr_env/lib/python3.10/site-packages/airflow/models/base.py:49 MovedIn20Warning: Deprecated API features detected! These feature(s) are not compatible with SQLAlchemy 2.0. To prevent incompatible upgrades prior to updating applications, ensure requirements files are pinned to "sqlalchemy<2.0". Set environment variable SQLALCHEMY_WARN_20=1 to show all deprecation warnings.  Set environment variable SQLALCHEMY_SILENCE_UBER_WARNING=1 to silence this message. (Background on SQLAlchemy 2.0 at: https://sqlalche.me/e/b8d9)
[[34m2023-10-27 17:11:57,731[0m] {[34mdagbag.py:[0m537} INFO[0m - Filling up the DagBag from /home/zhangs/airflow/dags/ptd_airflow.py[0m
[[34m2023-10-27 17:11:58,184[0m] {[34mtask_command.py:[0m376} INFO[0m - Running <TaskInstance: call_ptd.ptd_job scheduled__2023-10-26T18:00:00+00:00 [queued]> on host kscprod-data1.esr.cri.nz[0m
[[34m2023-10-27 17:12:05,119[0m] {[34mscheduler_job.py:[0m588} INFO[0m - Executor reports execution of call_ptd.ptd_job run_id=scheduled__2023-10-26T18:00:00+00:00 exited with status success for try_number 1[0m
[[34m2023-10-27 17:12:05,150[0m] {[34mscheduler_job.py:[0m631} INFO[0m - TaskInstance Finished: dag_id=call_ptd, task_id=ptd_job, run_id=scheduled__2023-10-26T18:00:00+00:00, map_index=-1, run_start_date=2023-10-27 04:11:58.299623+00:00, run_end_date=2023-10-27 04:12:04.817844+00:00, run_duration=6.518221, state=success, executor_state=success, try_number=1, max_tries=0, job_id=6, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-10-27 04:11:56.594411+00:00, queued_by_job_id=1, pid=467984[0m
[[34m2023-10-27 17:12:05,560[0m] {[34mdagrun.py:[0m597} INFO[0m - Marking run <DagRun call_ptd @ 2023-10-26 18:00:00+00:00: scheduled__2023-10-26T18:00:00+00:00, state:running, queued_at: 2023-10-27 04:11:56.406212+00:00. externally triggered: False> successful[0m
[[34m2023-10-27 17:12:05,560[0m] {[34mdagrun.py:[0m644} INFO[0m - DagRun Finished: dag_id=call_ptd, execution_date=2023-10-26 18:00:00+00:00, run_id=scheduled__2023-10-26T18:00:00+00:00, run_start_date=2023-10-27 04:11:56.464486+00:00, run_end_date=2023-10-27 04:12:05.560924+00:00, run_duration=9.096438, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2023-10-26 18:00:00+00:00, data_interval_end=2023-10-27 00:00:00+00:00, dag_hash=73809f00374e264528c5ff396bb50051[0m
[[34m2023-10-27 17:12:05,569[0m] {[34mdag.py:[0m3336} INFO[0m - Setting next_dagrun for call_ptd to 2023-10-27T00:00:00+00:00, run_after=2023-10-27T06:00:00+00:00[0m
[[34m2023-10-27 17:15:54,010[0m] {[34mscheduler_job.py:[0m1380} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2023-10-27 17:20:54,655[0m] {[34mscheduler_job.py:[0m1380} INFO[0m - Resetting orphaned tasks for active dag runs[0m
